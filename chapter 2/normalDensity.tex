
\documentclass[11pt]{article}
\usepackage{geometry}                % See geometry.pdf to learn the layout options. There are lots.
\geometry{letterpaper}                   % ... or a4paper or a5paper or ... 
%\geometry{landscape}                % Activate for for rotated page geometry
%\usepackage[parfill]{parskip}    % Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{epstopdf}
\DeclareGraphicsRule{.tif}{png}{.png}{`convert #1 `dirname #1`/`basename #1 .tif`.png}

\title{Notes: Normal Density }
\author{Dan Beatty}
%\date{}                                           % Activate to display a given date or no date

\begin{document}
\maketitle
%\section{}
%\subsection{}

\date{1-16-2007}

\begin{itemize}
	\item Multivariate Normal Density
	\item Gaussian Density
\end{itemize}

\begin{quote}
The case where the feature vector $\vec{x}$ for a given $\omega_i$ are continuous-value, randomly corrupted versions of a single typical/ prototype vector $\vec{\mu}$ \cite[31]{duda-hart-stork}
\end{quote}

\subsection{Expected values: }
\begin{eqnarray*}
	\Xi [f(x)] \equiv \int^{\infty}_{-\infty} f(x)p(x)dx \\
	\Xi [f(x)] = \sum _{x\in D} f(x)P(x)
\end{eqnarray*}
such that $P(x)$ is the probability mass at $x$.

\subsection{Univariate Density}
Let the notations be:
\begin{itemize}
	\item Mean: $\mu = \Xi [x]$
	\item variance: $\sigma^2 = \Xi [(\vec{x} - \vec{mu})]$
\end{itemize}
Three other notions are needed 
\begin{itemize}
	\item Entropy, which is a measure of the randomness or unpredictablity of a sequence of symbols drawn a discrete distribution.
	\item NAT/BIT are fundamental units of entropy.
	\item The Central Limit Theorem: ``the aggregate effect of the sum of a large number of small, independent random disturbances will lead to a Gaussian distribution.'' \cite[33]{duda-hart-stork} 
\end{itemize}
This knowledge is applied to the general univariate normal density function 
\begin{equation}
p(x) = \frac{1}{\sqrt{2\pi} \sigma} e^{\frac{1}{2}(\frac{x - \mu}{\sigma})^2 }
\end{equation}

\begin{itemize}
	\item Density is analytically simple
	\item The univariate case is considered for continuous density
	\item Most cases are considered Gaussian 
	\item Such a distribution has $95\%$ of its area within $2\sigma$ of its mean. 
\end{itemize}



\subsection{General Multivariate Normal Denisty $d$ Dimension}

\[
	p(\vec{x}) \equiv \frac{1}{(2\pi)^{\frac{1}{2}}  |\Sigma|^{-1}} e^ {- \frac{(\vec{x} - \vec{\mu})  \Sigma^-1 (\vec{x} - \vec{\mu})}{2}}
\]
\begin{itemize}
	\item $\vec{x}$ is a component vector
	\item $\vec{\mu}$ is a mean vector
	\item $\Sigma$ is a $d \times d$ covariance matrix
	\item $|\Sigma|$ determinant of $\Sigma$
	\item $\Sigma ^{-1}$ inverse of $\Sigma$
	\item $(\vec{x} - \vec{\mu})^T$ is the transpose of $(\vec{x} - \vec{\mu})$
	\item $\mu =\mathcal{E} [\vec{x}] $
	\item $\Sigma = \mathcal{E} [(\vec{x} -\vec{\mu})(\vec{x} -\vec{\mu})^T ] $
\end{itemize}

A notion from this density function is the notion of covariance.   A covariance matrix is always symmetric and positive semi-definite.  Most operations are on $\Sigma$ which is positive definite.  Certain conditions exist for non-determinism such as $|\Sigma| \equiv 0$ due to:
\begin{itemize}
	\item $p(\vec{x})$ is degenerate
	\item a component of $\vec{x}$ has zero variance
	\item two components are not linear independent.
\end{itemize}
The components of a covariance matrix are as follows:
\begin{itemize}
	\item The diagonal $\sigma_{ii}$ are variances of respective $x_i$, aka $\sigma_{ii} = (\sigma(x_i) ))^2$.
	\item The off diagonal elements $\sigma_{ij}$ are covariances of $x_i$ and $x_j$.
	\item $x_i$ and $x_j$ are statistically independent if $\sigma_{ij} \equiv 0$.
	\item If all $\sigma_{ij} \equiv 0$ for all $i \neq j$ then $p(\vec{x})$ reduces the product of the univariate densities for the components of $\vec{x}$.
\end{itemize}

There is an application of the whitening transform defined:
\begin{equation}
	\mathbf{A_{w}}  = \frac{\mathbf{\Phi} }{\sqrt{\mathbf{\Lambda}}}
\end{equation}
where 
\begin{itemize}
	\item $\mathbf{A_{w}}$ is matrix result of the whitening transform
	\item $\mathbf{\Phi}$ is a diagonal matrix containing the eigenvalues of $\Sigma$
	\item $\mathbf{\Lambda}$ is a matrix composed of the eigenvectors of $\Sigma$
\end{itemize}


There are some facts that have my question I do not see where the text obtains these conclusion:
\begin{itemize}
	\item How is the multivariate density reduced to $\frac{3d}{2}+ \frac{d^2}{2}$? \\
	The text claims that these are the number elements and these elements are derived from the mean vector $\mu$ and the independent elements of $\Sigma$ (the covariance matrix).  
	\item Presumably normal populations generate clusters.  The center of these clusters are claimed determined by the mean vector.  The shape of these are claimed to be formed by $\Sigma$.   
	\item How is the equation $\vec{a}^T \mathbf{B} \vec{a}$ a quadratic?  
	\item The quantity known as the Mahalonbois distance from $\vec{x}$ to $\vec{\mu}$
	\[
		r^2 = (\vec{x} - \vec{\mu})^T \mathbf{\Sigma_i} (\vec{x} - \vec{\mu})
	\]
\end{itemize}


\section{Discriminant Functions for the Normal Density}
\begin{itemize}
	\item Minimum error rate classification can be achieved by the discriminant function
	\item Case of multivariate normal 
	\begin{equation}
		g_i (\vec{x}) -\frac{1}{2}(\vec{x} - \vec{\mu}_i) ^T \mathbf{\Sigma_i} ^{-1} (\vec{x} - \vec{\mu}_i) - \frac{d}{2} \ln 2\pi - \frac{1}{2} \ln |\mathbf{\Sigma_i} | + \ln P(\omega_i) \label{noramlDenistyMultivariate}
	\end{equation}
	\item $\mathbf{\Sigma_i} = \sigma^2 I$  such that $I$ is the identity matrix equal in size to $\Sigma_i$.  \\
	In this case, the linear discriminant function is denoted as in equation \ref{discriminantFunctionScalarIdentity} and in terms of the threshold for the $i$th category (equation \ref{thresholdForTheIthCategory}) and equation \ref{wilineardiscriminant}.  THis case referred to as features statistically the same with the same variance.
	\begin{eqnarray}
	\vec{w}_i = \frac{1}{\sigma^2} \vec{\mu} _i \label{wilineardiscriminant} \\
	w_{i0} = -\frac{1}{2\sigma ^2} \vec{\mu}_i ^T \vec{mu}_i + \ln P(\omega_i)  \label{thresholdForTheIthCategory} \\
	g_i (\vec{x}) = \vec{w}^T \vec{x} + w_{i0} \label{discriminantFunctionScalarIdentity}
	\end{eqnarray}
	\begin{itemize}
		\item A classifier that uses linear discriminant function is called ``a linear machine''
		\item The decision surfaces for a linear machine are pieces of hyperplanes defined by $g_i(\vec{x}) =
	 g_j(\vec{x})$.
		% reference figure 2.11
	\end{itemize}
	\item Case $\mathbf{\Sigma_i} = \mathbf{\Sigma}$ (Identical non-arbitrary covariant classes)
	\begin{itemize}
		\item Hyperplane separating $\mathcal{R}_i$ and $\mathcal{R}_j$
		\begin{equation}
			x_0 = \frac{1}{2}(\vec{\mu} _i + \vec{\mu} _j) - \frac{\ln (P(\omega_i) / P(\omega_j))}{(\vec{\mu} _i - \vec{\mu} _j) ^T \Sigma^{-1} (\vec{\mu}_i - \vec{\mu} _j)  } (\vec{\mu} _i - \vec{\mu} _j) \label{hyperPlaneDerivedFromDiscriminant}
		\end{equation}
		leads to equation \ref{hyperPlaneDerivedFromDiscriminant}.  One hint supplied by \cite{duda-hart-stork} is that hyper-planes seperating $\mathcal{R}_i$ and $\mathcal{R}_j$ is generally not orthogonal to the line between the means!  % reference figure 2.12 
	\end{itemize}
	\item $\mathbf{\Sigma_i}$ is arbitrary
	\begin{itemize}
		\item The covariance matrices are different for each category.  %The threshold for the $i$th category is the same as when the features are statistically independent with the same variance.  
		\begin{eqnarray}
			w_{i0} = -\frac{1}{2}\vec{mu_i}^T \mathbf{\Sigma_i}^T \vec{mu_i} \label{thresholdForIthCategoryArbitraryCase}\\
			\mathbf{W}_i = -\frac{1}{2} \mathbf{\Sigma_i} ^{-1}  \\
			\vec{w}_i = \mathbf{\Sigma_i} ^{-1} \vec{\mu_i} \\
			g_i(\vec{x}) = \vec{x}^T \mathbf{W}_i \vec{x} + \vec{w}_i^T \vec{x} = \vec{w}_{i0} 
		\end{eqnarray}
		Hyper-quadratics consists of hyperplanes, hyper-spheres, hyper-spheres, hyper-ellipsoids, hyper-hyperbolis of various types.  % refer to figures 2.13, 2.14
		
	\end{itemize}
	
	
\end{itemize}



\section{Bayes Decision Theory - Discrete Features}

\begin{itemize}
	\item Components of $x$ are binary or integer valued, can take only one of $m$ discrete values
	\[
	v_1, v_2, ..., v_n
	\]
	\item Case of independent binary features in 2 category problem.  Let 
	\[x = [x_1, x_2, ..., x_d]^T \]
	where each $x_i$ is either 0 or 1, with probabilities:
	\begin{eqnarray}
		p_i = P(x_i 1 | \omega_1) \\
		q_i = P(x_i =1 | \omega_2)
	\end{eqnarray}
	\item The discriminant function in this case is:
	\begin{eqnarray}
		w_i = \ln \frac{p_i (1- q_i)}{q_i (1 - p_i)} \\
		w_0 = \sum_{i=0}^{d}  \ln \frac{1 - p_i }{1 - q_i} + \ln \frac{P( \omega_1) }{ P (\omega_2) } \\
		g(\vec{x}) = \sum _{i=1} ^d w_i x_i + w_0	
	\end{eqnarray} 
	The decision made from these equations chooses 
	\[
\left(
\begin{array}{ll}
\omega_1  &   g(x) > 0  \\
\omega_2  &   g(x) \ge 0  
\end{array}
\right)
\]

	
\end{itemize}



\bibliography{../patternNotes}
\bibliographystyle{abbrv}
\end{document}




