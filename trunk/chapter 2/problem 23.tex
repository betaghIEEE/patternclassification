\documentclass[11pt]{article}
\usepackage{geometry}                % See geometry.pdf to learn the layout options. There are lots.
\geometry{letterpaper}                   % ... or a4paper or a5paper or ... 
%\geometry{landscape}                % Activate for for rotated page geometry
%\usepackage[parfill]{parskip}    % Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{epstopdf}
\DeclareGraphicsRule{.tif}{png}{.png}{`convert #1 `dirname #1`/`basename #1 .tif`.png}

\title{HW problem 2-23}
\author{Dan Beatty}
%\date{}                


\date{2-1-2007}                           % Activate to display a given date or no date

\begin{document}
\maketitle
%\section{}
%\subsection{}


\begin{quote}
	Consider the three-dimensional normal distribution $p(\vec{x}|\omega) ~ N(\vec{\mu}, \mathbf{\Sigma})$ where 
	\begin{eqnarray}
		\vec{\mu} = 
		\left\{
		\begin{array}{l}
			1 \\
			2 \\
			2 
		\end{array}
		\right.
		\\
		\mathbf{\Sigma} = 
		\left\{
		\begin{array}{lll}
			1 & 0 & 0 \\
			0 & 5 & 2 \\
			0 & 2 & 5 
		\end{array}
		\right\}
	\end{eqnarray}
	\begin{enumerate}
		\item the probability density at the point $\vec{x_0}= (0.5,0,1)^T$.\label{partA}
		\item Construct the whitening transform $\mathbf{A_w}$.  Compute the matrices representing eigenvectors and eigenvalues, $\Phi$ and $\Lambda$.  Next, convert the distribution to one centered on with covariance matrix equal to the identity matrix, $p(\vec{x}|\omega) ~ N(0,\mathbf{I})$
		\item Apply the same overall transformation to $\vec{x_0}$ to yield a transformed point $\vec{x_w}$ .
		\item By explicit calculation, confirm that the Mahanlanobis distance from $\vec{x_0}$ to the mean $\vec{\mu}$ in the original distribution is the same as for $\vec{x_w}$ to $\mathbf{0}$ in the transformed distribution.  
		\item Does the probability density remain unchanged under a general linear transformation?  In other words, is $p(\vec{x_0}|N(\vec{mu} , \mathbf{\Sigma})) = p(\mathbf{T}^T \vec{x_0} | N(\mathbf{T}^T \vec{\mu}\mathbf{T}^T \mathbf{\Sigma T} ))$ for some linear transform $\mathbf{T}$?  Explain.
		\item Prove that a general whitening transform $\mathbf{A_w} = \mathbf{\Phi \Lambda}^{1/2} $ when applied to a Gaussian distribution ensures that the final distribution has covariance proportional to the identity matrix $\mathbf{I}$.  Check whether normalization is preserved by the transformation.  
	\end{enumerate}
	
\end{quote}



We are considering a 3 dimensional normal distribution problem.  Consider a 3-D normal distribution $p(\vec{x} | \omega) ~ N(\mu , \Sigma)$, such that 
\begin{eqnarray}
	\mu = 
	\left\{
	\begin{array}{l}
		1 \\
		2 \\
		2 
	\end{array}
	\right.
\\
\Sigma = 
\left\{
\begin{array}{lll}
	1 & 0 & 0 \\
	0 & 5 & 2 \\
	0 & 2 & 5 
\end{array}
\right\}
\end{eqnarray}


\section{PDF for $\vec{x_0}$ for part \ref{partA}}
This exercise was performed in Mathematica \cite{wolframResearch} with the values provided.   The PDF was computed via the classic PDF formula in the multivariate case as provided in equation \ref{multivariatePDF}.
\begin{equation}
	p(\vec{x}) \equiv \frac{1}{(2\pi)^{\frac{1}{2}}  |\Sigma|^{-1}} e^ {- \frac{(\vec{x} - \vec{\mu})  \Sigma^-1 (\vec{x} - \vec{\mu})}{2}} \label{multivariatePDF}
\end{equation}
The result provided by Mathematica was 0.00815733 .  

\section{Computing the Whitening Transform}
This section is mostly performed in Mathematica for expedience.  Notes are being kept to ensure that libraries developed from these exercises consider proper methods necessary for the properties and derivative operations needed for these structures.  

The equation for the whitening transform supplied via the errata page is 
\begin{equation}
	\mathbf{A_w} = \Phi \Lambda^{-1/2} \Phi ^T
\end{equation}
One difficulty encountered was the call of the meaning of $\Lambda^{-1/2}$.  $\Lambda$ is the diagonal matrix containing the eigenvalues.  The method to take the square root of a matrix is called the Cholesky Factorization. Fortunately, $\Lambda$ is diagonal as is.   Thus only the elements require the application of the square root operator, and the inverse is relatively easy for most math engines (Mathematica, Matlab, Maple, etc.)
\begin{equation}
	\mathbf{A_w} = 
	\left\{
	\begin{array}{lll}
	1.& 0 & 0 \\
	0 & 0.477657 & -0.0996929 \\
	0 & -0.0996929 & 0.477657
	\end{array}
		\right\}
\end{equation}
Also, Mathematica has a function called MatrixPower which provides interpretation of the power operation.  Thus both raising via matrix multiplication and lowering via Cholesky Factorization are provided.   




\section{Application of Whitening Transform to $\vec{x_0}$}
Part 23c, there is a whitening transform, and the transformed point 
\[
x_w = A^T (x_0 - \mu) \approx
\left\{
\begin{array}{l}
	-0.5 \\
	-1.71124 \\
	-0.556543
\end{array}
	\right\}
\]



\section{Confirmation of the Mahalanobis Distance}
\begin{quote}
By explicit calculation, confirm that the Mahanlanobis distance from $\vec{x_0}$ to the mean $\vec{\mu}$ in the original distribution is the same as for $\vec{x_w}$ to $\mathbf{0}$ in the transformed distribution. 
\end{quote}

\begin{eqnarray}
r^2 = (\vec{x} - \vec{\mu})^T \Sigma^{-1} (\vec{x} - \vec{\mu}) \approx 1.05952\\
{r_w}^2 = \vec{x_w} ^T  \vec{x_w} \approx 1.05952
\end{eqnarray}


\section{Is the probability affected by the Whitening Transform?}
\begin{quote}
Does the probability density remain unchanged under a general linear transformation?  In other words, is $p(\vec{x_0}|N(\vec{mu} , \mathbf{\Sigma})) = p(\mathbf{T}^T \vec{x_0} | N(\mathbf{T}^T \vec{\mu}\mathbf{T}^T \mathbf{\Sigma ^T} ))$ for some linear transform $\mathbf{T}$?  Explain.
\end{quote}
No, so long $\Phi$ is normalized.  If $\Phi$ is not normalized, then constants get inserted that fudge the results.  





\section{Proof}
\begin{quote}
Prove that a general whitening transform $\mathbf{A_w} = \mathbf{\Phi \Lambda}^{1/2} $ when applied to a Gaussian distribution ensures that the final distribution has covariance proportional to the identity matrix $\mathbf{I}$.  Check whether normalization is preserved by the transformation. 
\end{quote}




\bibliography{../patternNotes.bib}
\bibliographystyle{abbrv}
\end{document}


