
\documentclass[11pt]{article}
\usepackage{geometry}                % See geometry.pdf to learn the layout options. There are lots.
\geometry{letterpaper}                   % ... or a4paper or a5paper or ... 
%\geometry{landscape}                % Activate for for rotated page geometry
%\usepackage[parfill]{parskip}    % Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{epstopdf}
\DeclareGraphicsRule{.tif}{png}{.png}{`convert #1 `dirname #1`/`basename #1 .tif`.png}

\title{Notes: Bayes Formula }
\author{Dan Beatty}
%\date{}                                           % Activate to display a given date or no date

\begin{document}
\maketitle
%\section{}
%\subsection{}

\date{1-16-2007}
\section{Bayes decision rule and theorem:}
State of nature is a random variable, and is defined in terms of probability.  
\begin{itemize}
	\item Bayes theorem is based on conditional probability of an event occurring given evidence based on another event.  
	Joint probability of two events $A,B$ is expressed as 
	\[ 
	P (A|B ) \equiv P(B|A)
	\]
	\item Equivalences on probability sets
	\begin{eqnarray}
		\textrm{Joint probability } P(B|A) \equiv \frac{P(B|A) \times P(A)}{P(B)} \\
		\textrm{Intersection } P(A\cap B) \equiv P(A|B)P(B) \equiv P(B|A)P(A)\\
		P(A|B) \equiv \frac{P(B|A) P(A)}{P(B)} 
	\end{eqnarray}
	\item Decision rule with only the prior information \\
	decide $\omega_1$ if $P(\omega_1) > P(\omega_2)$ otherwise decide $\omega_2$
	\item Use of the class - conditional information \\
		$P(x | \omega_1)$ and $P(x | \omega_2)$ describe the difference in two separate populations. 
	\item \textsl{Bayes Formula for Continuous Sample Space:}  Let $X \to $ a feature space $R^d$  (feature space), $\omega _j$  represent a finite set of $C$ possible states classes of existence (i.e, $\omega_1, \omega_2, ..., \omega_c$), then the Bayes rule is 
		\[
		P(\omega_j | x) = \frac{p(x|\omega_j) P(\omega _j)} {p(x)}
		\]
		such that 
		\begin{itemize}
			\item $P(\omega_j | x)$ is the posterior (posteriori probability)
			\item $p(x| \omega_j)$ is likelihood of $\omega_j$ with respect to $x$
			\item $p(\omega_j)$ is prior
			\item $p(x)$ is the evidence factor which is a scaling (normalizing) factor.  $p(x)$ satisfies in the continuous case:
			\[
			\int ^{\infty}_{- \infty} p(x)dx \equiv 1
			\]
		\end{itemize}
		
	\item Posterior, likelihood, evidence
	\begin{itemize}
		\item $P(x)$	
	\end{itemize}
	
\end{itemize}

Even if there is a Normal Distribution, that is fine.  However, not all distributions are normal, and parameters can be found via Bayes Formula.  




$p(x) \to $   probability in the region $[ 0,1 ]$ in which 
\[
p(x) \sum^c _{j=1} p(x_j) \equiv 1
\]

$P(x) \to$ evidence is a scaling term (\cite[22]{duda-hart-stork}) 
\begin{eqnarray*}
	p(x) \equiv \int p(x | \omega _j) P(\omega) d \omega_j \\
	\sum p(x | \omega_j) p(\omega _j) \\
	p(\omega_j | x) \frac{p(x|\omega_j) p(\omega|j) }{\int^{\infty}{- \infty} p(x|\omega_j) p(\omega_j) d\omega_j} 
\end{eqnarray*}


In the discrete case, instead of feature vector $x$ being a point in d- dimensional space, $x$ assumes, say $m$ discrete values and all integrals are replaced by summations.  
\begin{eqnarray*}
	P(x) \equiv \sum ^C _{j=1} p(x|\omega_j) p(\omega _j) \\
	\textrm{Bayes rule } p(\omega_j |x) \equiv \frac{p(x|\omega_j) P(\omega_j)}{p(x)}
\end{eqnarray*}

Why is normal multivariate used in measuring Bayes or any other classifier?
Importance of assumption testing (? doubt generation)
\begin{itemize}
	\item BBN are measures of likelihood that an object comes from a specific class
	\item Normality is well studied and related principles are more easily understood.
	\item It has simple inference methods on parameters and differences between classes.
	\item Checks on data sets are necessary to check the fit to the data. 
\end{itemize}

``Bayes' rule will define the limiting performance that any suboptimal decision rule can attain.''  Decision rules are the key aspect desired from pattern recognition.  Such decision rules are typically used to construct the equivalent predicates and Horn clauses.   Of course, these rules can not be given the same strength as predicates as their source is not certain, where as Horn clauses presume that the facts are infallible. 

In fact, much study has gone into determining the total probability of misclassification.

\subsection{Risk factors} \cite[25]{duda-hart-stork}
Allowing the use of more than one feature merely requires replacing the scalar $x$ by $\vec{x}$  where 
\begin{itemize}
	\item $\vec{x}$ is a feature vector
	\item $\vec{x} \in \mathrm{R}^d$ 
	\item $\vec{x}$ is in a $d$-dimensional 
	\item ($\mathbf{R}^d$) is an Euclidean space called a feature space.
\end{itemize}

\textbf{Loss function:} the loss function states exactly how costly each action is, and is used to convert a probability determination into a decision.  

An expected loss is called a risk $R(\alpha_i | x)$ is the notation for a conditional risk.  Using Bayes formula, conditional risk can be derived to:
\[ 
R(\alpha_i | \vec{x}) \equiv \sum _{j=1} ^c \lambda (\alpha_i | \omega_j) P(\omega_j | \vec{x})
\]
The Bayes decision procedure is used to minimize risks.  


%{Bayes Discriminant Rule}


%(side note: registration issue)

A framework for the Bayes Network must include prior probabilities, posterior probabilities, and likelihood and evidence probabilities.  It must also contain risk computations.    From these, we have one classification and typically there are several classifications (one for each class).


``It is important  to know that the optimality of a Bayesian decision rule is independent of the form of the distribution.''\cite{nadler}  Its purpose is to minimize the probability of misclassification.   Equation 7.9 gives us means of calculating the total probability of misclassification.

\section{Bayesian Decision Theory}
\begin{itemize}
	\item Minimum Error Rate Classification
	\item Classifiers, Discriminant Functions and Decision Surfaces
	\item The Normal Density
\end{itemize}

\begin{itemize}
	\item Actions are decisions on classes \\
	If action $\alpha_i$ is taken and the true state of nature is $\omega _j$ then: the decision is correct if $i\equiv j$ and error if $i \neq j$.
	\item Seek a decision that minimizes the probability of error which is the error rate.
	\item Introduction of the zero-one loss function
	\[ 
		\lambda (\alpha_i, \omega_j) = 
		\begin{array} {ll}
			0 & i \equiv j \\
			1 & i \neq j 
		\end{array}	
	\]
	where the domain of $i,j \in [1,c]$.  Therefore, the conditional risk is 
	

	\begin{eqnarray*}
	\mathbf{R}(\alpha_i | x) \sum ^{c} _j=1 \lambda (\alpha_i | \omega_j) P(\omega_j | x) \\
	= \sum _{j \neg 1} P(\omega _j | x) = 1 = P(\omega_i | x) 	
	\end{eqnarray*}
	``The risk corresponding to this loss function is the average probaility error''
	\item Minimize the risk requires maximize $P(\omega_i | x)$ (since $R(\alpha_i | x) = 1 - P(\omega_i | x)$)
	\item For Minimum error rate \\
	decide $\omega_i$ if $P(\omega_i | x) > P(\omega_j | x) \forall j \neq i$
	\item Regions of decision and zero-one loss function, therefore: 
	Let $\frac{\lambda_{12} - \lambda_{22}}{\lambda_{21} - \lambda_{11}} \cdot \frac{\vec{P}(\omega_2)}{\vec{P}(\omega_1)} \equiv \theta_k$ then decide $\omega_1$ if $\frac{\vec{P}(\omega_1)}{\vec{P}(\omega_2)} > \theta_k$
	\item If $\lambda$ is the zero-one loss function which means:
	
\end{itemize}

``It is reasonable to assume that errors cost more than correct decisions.''\cite{nadler}  This is the purpose of defining regions of decision that minimize loss. In other words the decision matrix discretizes the posteriori values with the normalized risk values, and due to monotonic niceness we use the log of this function to unsure only positively increasing categories.  

\begin{tabular}{cc}
\hline
Continuous & Discrete\\
\hline
$P(\omega_j | x ) \frac{p(x| \omega_j) P(\omega_j)}{p(x)} $ &
$P(\omega_j | x ) \frac{p(x| \omega_j) P(\omega_j)}{p(x)} $ \\
\hline
\end{tabular}




Loss function:  The error in making a decision regarding assigning a data $x$ to a particular category is evaluated by  means of an expected loss or risk:

\begin{eqnarray*}
\mathbf{R}(\alpha_i | x) \sum ^{c} _j=1 \lambda (\alpha_i | \omega_j) P(\omega_j | x) \\
= \sum _{j \neg 1} P(\omega _j | x) = 1 = P(\omega_i | x) 	
\end{eqnarray*}
where
\begin{itemize}
	\item $\mathbf{R}(\alpha_i | x)$ is the conditional risk, $\lambda ( \alpha_i | \omega_j)$ is the loss incurred by taking an action $\alpha_i$ for assigning a particular  $x$ to the class $\omega_j$ and $p(\omega_j | x)$ is the probability of $x$ belonging to $\omega_j$	
\end{itemize}
	
	
Consider a simple two-category case: 
\begin{eqnarray*}
\mathrm{R}(\alpha_i |x ) = \lambda_{11}P(\omega_1 | x) + \lambda_{12} P(\omega_2 | x) \\
\mathrm{R}(\alpha_2 |x ) = \lambda_{21}P(\omega_1 | x) + \lambda_{22} P(\omega_2 | x) 
\end{eqnarray*}
If $R(\alpha_1 | x) < R(\alpha_2 |x)$, then $\omega_1$ is the right category to choose Now 
\begin{eqnarray}
R(\alpha_1 | x ) - R ( \alpha_2 | x) \equiv -(\lambda _{21} - \lambda_{11}) P(\omega_1 | x) + ( \lambda _{12} - \lambda_{22}) P(\omega_2)  \\
(R(\alpha_1 | x) < R(\alpha_2 |x)) \equiv ((\lambda_{11}P(\omega_1 | x) + \lambda_{12} P(\omega_2 | x)) < (\lambda_{21}P(\omega_1 | x) + \lambda_{22} P(\omega_2 | x) )) \\
\equiv ((\lambda_{11}P(\omega_1 | x) + \lambda_{12} P(\omega_2 | x) - \lambda_{21}P(\omega_1 | x)) < (\lambda_{22} P(\omega_2 | x) ) ) \\
\equiv (((\lambda_{11} - \lambda_{21})P(\omega_1 | x) ) < \lambda_{22} P(\omega_2 | x) )  - \lambda_{12} P(\omega_2 | x)) \\
\equiv (((\lambda_{11} - \lambda_{21})P(\omega_1 | x) ) < (\lambda_{22} - \lambda_{12}) P(\omega_2 | x)  
\end{eqnarray}
	
\section{Minimum-Error Rate Classification}
Classification problems define each state of nature with associations between classes and actions.  For notational purposes 
\begin{itemize}
	\item classes are denoted by $c$
	\item the action $\alpha_i$
	\item state of nature $\omega_j$
	\item $\lambda(\alpha_i | \omega_j)$ is a loss function that quantifies exactly how costly each action is. 
	\item Minimum error rate is defined:
	\begin{equation}
		\lambda (\alpha | \omega_j) =
	\left\{
	\begin{array}{ll}
	0  &    i \equiv j , i,j = 1 , ..., c \\
	1  &    i \neq j  \\
	\end{array}
	\right.
	\end{equation}
	\item The risk corresponding to this loss function is defined:
	\begin{eqnarray}
		R(\alpha_i | \vec{x}) \equiv \sum ^c _{j=1}  \lambda (\alpha_i | \omega_j) P(\omega_j | \vec{x})  \\
		\equiv \sum _{j\neq i} P (\omega_j | \vec{x})
	\end{eqnarray}
	Note that Risk is defined by average probability, thus 
	\[ 
		R(\alpha | \vec{x}) \equiv 1 - P(\omega_i | \vec{x}) 
	\]
	\item Watch out for cut offs of the region functions.  These functions obey the rejection function.
\end{itemize}



%Issue for minimum error rate:   A symmetrical zero-loss function:   

%Minimum error rate \cite[26]{duda-hart-stork}

Therefore, to minimize the conditional risk $R(\alpha_i | x)$ , one needs to maximize the posterior probability $P(\omega_i | x)$.   This is called the minimum error rates 
so choose $\omega_i$ if 
\[ 
P(\omega_i | x ) > P(\omega_j | x), i \neq j
\]

\subsection{Minimax Criterion}
Some classifiers to perform well over a range of prior probabilities.  Reasonable approaches design classifiers to minimize the worst overall risk for any value of the priors.

If we can find a bounding such that the constant of proportionality is zero, then the risk is independent of priors.  

\subsection{Neyman-Pearson Criterion}
\begin{quote}
	We may wish to minimize the overall risk subject to a constraint.  
\cite[33-34]{duda-hart-stork}
\end{quote}
The Neyman-Pearson criterion is typically computed by adjusting the decision boundaries numerically.  


Bayes rule allows us to compute the posterior probabilities from the prior probabilities $p(\omega_j)$ and the conditional densities 
\[ 
p(x | \omega_j)
\]
The parameters may not be known, and presents a weakness for the Bayes rule cases.  


In one case, we assume the densities to be multi-variate normal (Gaussian).  


Discriminate functions are defined for classifying patterns $g_i(x) = p(\omega_i | x)$ is the discriminant function.  \cite[29-30]{duda-hart-stork} (equations 26- 28)

In univariate case, the normal density is given by (formula 34 \cite[32]{duda-hart-stork}) 


In the multi-variate case (formula 38 and 41 \cite[33-34]{duda-hart-stork})

$\Sigma \to d \times d$  covariance matrix and $|\Sigma|$ and $\Sigma^{-1}$ are determinant and inverse of $\Sigma$


In the general multivariate normal case, the covariance matrices $\Sigma_{i}$, are different for each category.   We assume then $\Sigma_i = $ arbitrary.   (case 3)

from pages 36-41


Example page 44:  

Reference HW Problem 14, can be found in many pattern classification problems one has the option of either assigning the pattern to one of the $C$ classes or reject it as being unrecognizable.  


Reference problem 13
Let 
\begin{eqnarray*}
\lambda (\alpha_i | \omega_j) = 
\begin{array}{ll}
0 & i \equiv j \\
\lambda_ r &  i \equiv  c \\
\lambda_s & \textrm{ otherwise} 
\end{array} \
\end{eqnarray*}



Let us choose $\omega_{max}$ as having the maximum posterior probability.  Our risk at a point $x$ is then:
\[ 
\lambda _s = \Sigma_{j \neq \textrm{max}} P( \omega_j | x) = \lambda_s [1 - p ( \omega_{\textrm{max}} |x)] 
\]
where if we reject, our risk $\to \lambda _\gamma$  If we choose a non-maximum category, $\omega_k$, where $k \neq \textrm{max}$ , then our risk is 
\[ 
	\lambda_s \sum _{j \neq k} P(\omega_j | x ) = \lambda_s [1 - p ( \omega_k | x)] \ge \lambda _s [1 - p ( \omega_{max} |x )]
\]
Therefore, we should choose either $\omega_{max}$ or we should reject depending on which is smaller.  
\[
\lambda_s [1 - p (\omega_{max} | x)] 
\]
or $\lambda_{\gamma}$ .

We reject if $\lambda_{\gamma} \le \lambda_s [1 - p (\\omega_{max} | x)]$  or $p(\omega_{max} | x) \ge [1 - \frac{\lambda_{\gamma}} {\lambda{s}}]$.


Problem 23

reference page 35   

Chapter 3 for maximum-likelihood and bayesian parameter estimation 

``ART'' 

Watch out for Neural Network example:   


In image processing, there is a use for ``CART''





\bibliography{../patternNotes}
\end{document}

	