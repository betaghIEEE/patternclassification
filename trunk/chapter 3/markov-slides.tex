
\documentclass[11pt]{article}
\usepackage{geometry}                % See geometry.pdf to learn the layout options. There are lots.
\geometry{letterpaper}                   % ... or a4paper or a5paper or ... 
%\geometry{landscape}                % Activate for for rotated page geometry
%\usepackage[parfill]{parskip}    % Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{epstopdf}
\DeclareGraphicsRule{.tif}{png}{.png}{`convert #1 `dirname #1`/`basename #1 .tif`.png}

\title{Notes on Markov Chains to Pattern Classification}
\author{Dan Beatty quoting Dr. Mitra}
\date{2-19-2007}                                           % Activate to display a given date or no date

\begin{document}
\maketitle


\section{Hidden Markov Models}

\begin{itemize}
	\item Markov Chains
	\item Goal: make a sequence of decisions
	\begin{itemize}
		\item Processes that unfold in time, states at time $t$ are influence by a state at $t-1$
		\item Applications: speech recognition, gesture recognition, parts of speech tagging and DNA sequencing, 
		\item Any temporal process without memory $\omega ^T \{ \omega(1),\omega(2), ..., \omega(T) \}$ sequence of states.  We might have $\omega^6 = \{ \omega 1, \omega 4, \omega 2, \omega 2, \omega 1, \omega 4  \}$.
		\item The system can revisit a state at different steps and not every state need to be visited 
	\end{itemize}
	\item First Order Markov models
	\begin{itemize}
		\item Our productions of any sequence is described by the transition probabilities. 
		\begin{eqnarray}
			P( \omega_j (t+1) | \omega_i (t)) = a_{ij} \\
			\theta = ( a_ {ij} , \omega^T) \\
			P(\omega^T | \theta) = a_{14} a_{42} a_{22} a_{21} a_{14} P(\omega(1) = \omega_i)
		\end{eqnarray}
		% cite figure 3-8
		\item Examples include speech recognition
		``production of spoken words''  Production of the word: ``pattern'' represented by phonemes.
		Transitions from p to a, a to tt, tt to er, and er to n to silent state.  
	\end{itemize}
	
\end{itemize}


note moon book

\section{The Hidden Markov Model}
A hidden Markov Model (HMM) is represented by the compact notation 
\begin{equation}
\lambda = ( \Lambda, B, \pi)
\end{equation}
for discrete probability distributions.  Here $\Lambda = \{ a_{ij} \} $ (often denoted $A$) is a set of state transition probabilities:  
\begin{equation}
a_{ij} = p ( q_{t+1} = j | q_t = i) , 1 \le i,j \le N
\end{equation}
$q_t \to$ the current state, so $a_{ij}$ is the probability of transition from the state $q_t \to q_{t+1}$.  

$a_{ij}$'s must satisfy the following constraints:
\begin{eqnarray}
	a_{ij} > 0 , 1 \le i, j \le N \\
	\sum_{j=1} ^N a_{ij} =1 , 1 \le i \le N
\end{eqnarray}
such that $N$ is the number of states.  Probability distribution in each of the state denoted:
\begin{eqnarray}
	B = \{ b_j (k)  \} \\
	b_j (k ) p( \theta_t = \gamma _k | q_t = j) , i \le j \le N , 1 \le k \le M 
\end{eqnarray}
where $\gamma_k$ is the $k$th observation symbol, and $\theta_t$ is the current parameter vector $b_j(k)$ must satisfy:
\begin{eqnarray}
	b_j (k) \le 0 , i \le j \le N, 1 \le k \le M \\ 
	\sum _{k=1}^M b_j (k) = 1 , 1 \le j \le N 
\end{eqnarray}


\end{document}