\section{Objective-C Version of the MDA Classifier}
In order to satisfy the Multiple Discriminant Analysis (MDA) section of the problem, this solution has taken on two forms.  One, a solution in Mathematica, solve the preconditioner by hand and shows demonstration of concept.  The other is an automated library to be service that can be used in general.  Lessons from the first provided means for the unit tests of the second.  For the sake of this discussion data class shall refer set for classifying the data into.  Object class shall refer to the Objective-C structure used to construct objects. 

The classic definition of Multiple Discriminant Analysis is defined by Algorithm \ref{alg:multiple_discriminant_analysis}.  In this definition, $D_i$ are the data classes, $\vec{m_i}$ are the sample mean of these data classes, $n_i$ are the number of samples in these data classes, $\vec{m_t}$ is the total mean for the samples of all of the data classes, and 

\begin{algorithm}
\caption{Multiple Discriminant Analysis}
\label{alg:multiple_discriminant_analysis}
\begin{algorithmic}
	\STATE Determine $\vec{m_t}$
	\FORALL{Classes $D_i$ in Discriminant Set $D$}
		\STATE Compute $\vec{m_i}$
		\STATE Determine $n_i$
		\STATE Determine $\hat{m_i} = \vec{m_i} - \vec{m_t}$
		\STATE Compute $S_i = \sum_{\vec{x_i} \in D_i} (\vec{x_i} - \vec{m_i} )(\vec{x_i} - \vec{m_i} )^T $
	\ENDFOR
	\STATE $S_w = \sum _{S_i \in D} S_i$
	\STATE Compute $S_B = \sum_{\hat{m_i} \in D} n_i \hat{m_i}$
	\STATE Compute Top eigenvectors for equation: 
	\[
	\mathbf{S_B} \mathbf{w_i} = \lambda_i \mathbf{S_W} \mathbf{w_i}
	\]
	\RETURN {$\mathbf{W}, \mathbf{\Lambda} $}
\end{algorithmic}
\end{algorithm}


The MDA is all about conditioning the data to one less component than there is a data class.  The components are usually arranged as the columns of a matrix, $\mathbf{W}$.  If the original data is arranged as row vectors in a matrix $\mathbf{X}$, then
\begin{equation}
\mathbf{XW} = \mathbf{Y}
\end{equation}
where $\mathbf{Y}$ is a matrix the same number of rows, $m$, as sample matrix and $c-1$ columns where $c$ are the number of data classes.  Each of the columns are mappings onto an ortho-normal basis.  

The Objective-C version has two constructors.  One takes an array of ``sample classes'' which are simply structures containing the original samples, the mean, covariance, and scatter matrix.  The other takes a matrix containing the samples and a vector containing a enumerated mapping of row vector to class.    In the second, the ``sample classes'' are constructed so that the preconditioning matrix $W$ may be computed.  The preconditioning matrix $W$ is defining feature of the object class, and should be the thing requested from the structure.  

In order to construct the preconditioning matrix, there are two sets of methods based on data classes (sample classes) to generate the matrices required.  These two matrices are the background scatter matrix ($\mathbf{S_b}$) and whitening scatter matrix ($\mathbf{S_w}$).  

Compute Whitening Scatter is derived on the basis that a data class structure produces a scatter matrix on initialization.  Thus computing whitening scatter matrix is merely an addition of all of the scatter matrices.  

\begin{equation}
\mathbf{S_w} = \sum _i \mathbf{S_i}
\end{equation}

In order to construct the background scatter matrix, one needs a few supplemental matrices.  Most mathematical definitions of MDA do not state this explicitly, yet it is a necessary step for efficiency.  

The computation for the scatter matrices used to construct the pre-conditioner is as follows:
\begin{itemize}
	\item Number of Samples Vector $\vec{n}$
	\item Number of Samples Matrix $\mathbf{N}$
	\item Mean Transformation Matrix $\mathbf{M}$
\end{itemize}
The background scatter matrix is defined from these structures as:
\begin{equation}
	\mathbf{S_b} = (\mathbf{N} \cdot \mathbf{M})(\mathbf{M}^T) 
\end{equation}
where the cdot indicates a dot-Multiply between $\mathbf{N}$ and $\mathbf{M}$.  %This method allows the classic definition of the background scatter matrix to be performed as a consequence 

Real trick is how the eigenvectors are computed for S_w and S_b. In this case, it is convenient to note the characteristic equation being solved.
\begin{eqnarray}
\mathbf{S_B}\vec{e_i} = \lambda_i \mathbf{S_w}\vec{e_i} \label{character_mda}\\
\mathbf{S_w}^{-1}\mathbf{S_B}\vec{e_i} = \lambda_i \mathbf{S_w}^{-1} \mathbf{S_w}\vec{e_i} \\
\mathbf{S_w}^{-1}\mathbf{S_B}\vec{e_i} = \lambda_i \mathbf{I} \vec{e_i} \\
\mathbf{S_w}^{-1}\mathbf{S_B}\vec{e_i} = \lambda_i \vec{e_i} \label{character-mda-singlematrix}\\
\end{eqnarray}
Most would argue that solving equation \ref{character_mda} would be the proper choice.  However, the Objective-C version does not have such a method.  It can compute inverses of square matrices, and eigenvectors of the characteristic equation \ref{eigenvector-character} which is the same as equation \ref{character-mda-singlematrix}.
\begin{equation}
\mathbf{A} \vec{e_i} = \lambda_i \vec{e_i} \label{eigenvector-character}
\end{equation}

 