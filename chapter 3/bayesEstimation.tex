\documentclass[11pt]{article}
\usepackage{geometry}                % See geometry.pdf to learn the layout options. There are lots.
\geometry{letterpaper}                   % ... or a4paper or a5paper or ... 
%\geometry{landscape}                % Activate for for rotated page geometry
%\usepackage[parfill]{parskip}    % Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{epstopdf}
\DeclareGraphicsRule{.tif}{png}{.png}{`convert #1 `dirname #1`/`basename #1 .tif`.png}

\title{Notes on Bayes Estimation Approach to Pattern Classification}
\author{Dan Beatty quoting Dr. Mitra}
\date{2-19-2007}                                           % Activate to display a given date or no date

\begin{document}
\maketitle
%\section{}
%\subsection{}


\begin{itemize}
	\item Bayesian Estimation (Bayesian Learning to pattern classification problems)
	\begin{itemize}
		\item In MLE, $\theta$ was supposed fixed
		\item In $\theta$ is a random variable
		\item From Bayes decision rule that was used in chapter 2
		\begin{eqnarray}
		p(\omega_i | x, \mathcal{D}) = \frac{p( x | \omega_i, \mathcal{D}) P(\omega_i | \mathcal{D})} {\sum p (x| \omega_j , \mathcal{D}) P (\omega_j | \mathcal{D})} \\
		\textrm{assumed } P (\omega_i) = p ( \omega_i | \mathcal{D})
		\end{eqnarray}
		In other words, the prior probabilities are unknown.  We use a set of $\mathcal{D}$ samples drawn independently from the fixed but unknown $p(x)$ to find $p(x| \mathcal{D})$ for $c$ separate classes.  

		It is assumed that $p(x)$ has a known parameter form $p(x|\theta)$, where the value of the parameter vector $\theta$ is unknown.
		\item The computation of posterior probabilities $P(\omega_i | x)$ and class-conditional densities $p(\vec{x} | \omega_i)$ lies at the heart of Bayesian classification.
		\item Goal: compute $P(\omega_i | x , D)$ given the sample $D$, Bayes formula can be written \cite[91]{duda-hart-stork}
		\begin{equation}
			P(\omega_i | \vec{x} , D) = \frac{P ( \vec{x}| \omega_i, D  ) P(\omega_i | D)} {\sum_{i=1}^c (P(\vec{x}| \omega_j, D) P(\omega_j |D)  ) } \label{posteriorProbabilities}
		\end{equation}
		To demonstrate the preceding equation, use:
		\begin{eqnarray}
			P(\vec{x}, D | \omega_i) = P(\vec{x} | D, \omega_i) \\
			P( \vec{x} |D ) = \sum_j P(\vec{x}, \omega_j | D) \\
			P(\omega_i) = P(\omega_i | D) \label{trainingSampleBayes}
		\end{eqnarray} 
		There are a few observations that lead to a useful computation:
		\begin{itemize}
			\item Each class $c_i$ is worked on separately with one sample $D_i$ to determine $p(\vec{x} | \omega_i , D)$.
			\item Equation \ref{trainingSampleBayes} translates equation \ref{posteriorProbabilities} (aka the training sample) to %comes from the training sample.  The derivation leads to 
			\begin{equation}
				P(\omega_i | \vec{x}, D) = \frac{P ( \vec{x} | \omega_i , D_i) P(\omega_i) } {\sum_{j=1}^c P(\vec{x} | \omega_j , D) P(\omega_j)}
			\end{equation}
			% Where do these equations come from.  They are similar to the definition of Bayes Formula, but different enough to ask where it comes from.  
			% Where do these equations come from.  They are similar to the definition of Bayes Formula, but different enough to ask where it comes from.  
			%This equation 31 and 32 \cite{duda-hart-stork} is derived via 29
			
		\end{itemize}
	\end{itemize}
	\item $p(\vec{x})$ may be unknown, but one assumption claims its form is parametric.  In the parametric form, it can be said that  $p(\vec{x}| \theta)$ is known.   Preferably, the goal includes acquiring the probability density function in Gaussian cases.  Using the joint density to obtain $p(\vec{x} | D)$: 
	\begin{eqnarray}
		p( \vec{x} | D) = \int p( \vec{x} , \vec{\theta} | D ) d \vec{\theta} \\
		= \int p( \vec{x} | \theta) p( \vec{\theta} | D) d \vec{\theta} \label{class-conditional-posterior-density}
	\end{eqnarray}
	\item How does this procedure provide a method to approximate the parameters $\vec{\theta}$?
	 \cite[92]{duda-hart-stork}
	\begin{itemize}
		\item Equation \ref{class-conditional-posterior-density} and  links class conditionals $p(\vec{x} | D)$ to posterior densities $p(\theta |D)$, though $\vec{\theta}$ is unknown. 
		\item Sharp peaks in the posterior density indicate an approximation the unknown parameter. 
		\item The approximation assumes that $p(\vec{x}| \vec{\theta})$ is generally smooth, an the outliers are insignificant.  
		\item Less certain peaks indicate the need for averaging of $p(\vec{x}| \vec{\theta})$
	\end{itemize}	
\end{itemize}

\section{Bayesian Parameter Estimation: Gaussian Case}
\textbf{Goal:} Estimate $\theta$ using the a-posteriori density $P(\theta | D)$.


In MLE approach, we assume the true parameter vector and to be fixed whereas in Bayesian approach we consider $\theta$ to be a random variable and we use training data to convert the distribution of $\theta$ into a posterior probability density $[P(\omega_i | x)]$.  Using a set of samples $\mathcal{D}$, the posterior probability $p(\omega_i | x, \mathcal{D})$ can be complete.



\begin{itemize}
	\item Univariate case: $P(\mu | D)$, $\mu$ is the only unknown parameter 
	\begin{eqnarray}
		P( x | \mu) ~ N( \mu , \sigma^2) \\
		P(\mu) ~ N( \mu_0 , \sigma_0^2 ) 
	\end{eqnarray}
	Such that $\mu_0$ and $\sigma_0 ^2$ are the best guesses for the mean and variance. The critical assumption is that the prior distribution for $\mu$ is known.  
	\begin{eqnarray}
		p(D | \mu) = \prod _{k=1}^n p(x_k | \mu) \\
		P(\mu | D) = \frac{P( D| \mu) P(\mu)}  {\int P(D | \mu) P(\mu) d\mu} \label{bayesNSamplesUnivariate}  \\
%		p(\omega_i | x, \mathcal{D}) = \frac{p( x | \omega_i, \mathcal{D}) P(\omega_i | \mathcal{D})} {\sum p (x| \omega_j , \mathcal{D}) P (\omega_j | \mathcal{D})} \\
		= \alpha \prod _{k=1} ^n P( x_k | \mu) P(\mu) %\\
%		\textrm{assumed } P (\omega_i) = p ( \omega_i | \mathcal{D}) 
	\end{eqnarray}
	In addition to using Bayes formula, the value for $\alpha$ is defined as a ``normalization factor that depends on $D$ but is independent of $\mu$''  A few equations need to be recalled in order to further derive the reproducing density.
	\begin{eqnarray}
		p(x_k | \mu) = \frac{1} {\sqrt{2\pi} \sigma} \exp [ -\frac{1}{2}(\frac{x_k - \mu}{\sigma}^2) ]  \label{pxkmu-bayesian-parameter-estimation}\\
		p( \mu) = \frac{1}{\sqrt{2\pi} \sigma_0} \exp [-\frac{1}{2} ( \frac{\mu - \mu_0}{\sigma_0}) ^2]  \label{pmu-bayesian-parameter-estimation}\\
		p( \mu | D ) ~ N( \mu_n , \sigma_n ^2) \label{reproducingDensityUnivariate} 
	\end{eqnarray}
	
	The reproducing density is derived using the equations remembered in equations \ref{pxkmu-bayesian-parameter-estimation} and \ref{pmu-bayesian-parameter-estimation}
	\begin{eqnarray}
		p( \mu | D ) = \alpha \prod _{k=1}^n  \frac{1} {\sqrt{2\pi} \sigma} \exp [ -\frac{1}{2}(\frac{x_k - \mu}{\sigma}^2) ] \frac{1}{\sqrt{2\pi} \sigma_0} \exp [-\frac{1}{2} ( \frac{\mu - \mu_0}{\sigma_0}) ^2] \\
		 = \alpha' exp [ - \frac{1}{2} ( \sum_{k=1}^n (\frac{\mu - x_k}{\sigma}^2) + (\frac{\mu - \mu_0}{\sigma_0}^2) ) ] \\
		= \alpha '' \exp [- \frac{1}{2} ( \frac{n}{\sigma^2} + \frac{1}{\sigma_0 ^2}) \mu^2 - 2 (\frac{1}{\sigma^2} \sum_{k=1} ^n x_k  + \frac{\mu_0}{\sigma_0 ^2})\mu ] 
	\end{eqnarray}
	\begin{itemize}
		\item $p(\mu)$ denotes the conjugate prior
		\item $\alpha , \alpha' , \alpha''$ are all normalizing factors.
		\item $p(\mu | D)$ denotes the reproducing density and is also called the posteriori density for the mean.
		\begin{itemize}
			\item it is quadratic
			\item if $p(x_k| \mu)$ is normal, then so is $p(\mu | D)$ ????
		\end{itemize}
		
	\end{itemize}
	
	Identifying equation \ref{bayesNSamplesUnivariate} and \ref{reproducingDensityUnivariate} yields:
	\begin{eqnarray}
		\mu _n = ( \frac{n \sigma_0^2}  {n_0 \sigma_0 ^2 + \sigma^2}  ) \hat{\mu}_n + (\frac{\sigma^2} {n \sigma_0 ^2 + \sigma^2}) \mu_0  \label{mu-learned-gaussian-case}\\
		\sigma_n ^2 = \frac{\sigma_0 ^2 \sigma^2} {n \sigma_0 ^2 + \sigma^2} \label{sigma-learned-gaussian-case}
	\end{eqnarray}
	% cite figure 3.2
%	\item $p( \mu | D)$ is obtained via these equations
	\item Consider the problem of learning the mean of a univariate normal distribution which are supplied (equation \ref{mu-learned-gaussian-case} \ref{sigma-learned-gaussian-case}\cite[94]{duda-hart-stork})  
	\begin{itemize}
		\item $\mu_n$ is sample mean of the training samples.  
		\item $\mu_0 $ ``is our best prior guess for $\mu$''
	\end{itemize}
	%\item $\mu_n$ denotes the best guess of $\mu$ after observing $n$ samples. 
	\item Dogmatism is the ratio of $\sigma_0 ^2$ and the actual variance $\sigma^2$.
	If dogmatism is not infinite, then
	\begin{itemize}
		\item  the initial guesses are not significant with sufficient samples.
		\item  $\mu_n$ will converge on the sample mean.
		\item Reference section \ref{problem_3_15}
	\end{itemize}
	From equations 34, 35 we have 
	%\begin{eqnarray}
	%\mu _n = \frac{n \sigma_0^2}{n \sigma_0 ^2 + \sigma^2} m_n + \frac{\sigma^2}
	%\end{eqnarray}

	%Observe camera from phone

	$\mu_0$ is formed by averaging $n_0$ fictious samples $x_k$ from 
	\[
	k = - n_0 + 1, - n_0 + 2, ..., 0 
	 \]
	Therefore 
	\begin{eqnarray}
		\mu_0 \frac{1}{n_0} \sum_{-n+1}^0 x_k \\
		\mu_n = \frac{\sum_{k=1}^n x_k }{n+ \frac{\sigma^2} {\sigma_0 ^2}} + {\sigma^2/\sigma_0^2} \sigma^2 /  \sigma \\
		\mu_n = \frac{\sum_{k=1}^n x_k }{n+ n_0} + \frac{n_0}{n + n_0} (\frac{1}{n_0})\sum_{k=1- n}^0 x_k \\
		n_0 = \frac{\sigma^2 }{\sigma_0 ^2} \\
		\mu _n = \frac{1}{n+n_0} [ \sum_{k=1}^n x_k + \sum_{}] ... 
	\end{eqnarray}
	Similarly $\sigma^2 _n $ 
	\begin{eqnarray}
		\sigma^2 _n = 
	\end{eqnarray}
	These results can be interpreted as follows:  For a suitable choice of the prior density, $p(\mu) ~ N(\mu_0 , \sigma^2 _0 )$ MLE on the full sample $(n+n_0)$ observations coincides with Bayesian inference on the second sample $n$ observations.
	\item The univariate case $P(x | D)$
		
\end{itemize}



\subsection{The univariate case $P(x | D)$}
\begin{itemize}
	\item Assume that $P( \mu | D)$ computed 
	\item Class conditional denoted $P(x | D)$ remains to be computed!  
	\begin{eqnarray}
		P(x | D) =  \int P(x | \mu) P( \mu | D) d\mu \label{univariateEvidence}\\
		= \int \frac{1}{\sqrt{2\pi} \sigma} \exp [-\frac{1}{2} (\frac{x-\mu}{\sigma})^2 ] \frac{1}{\sqrt{2\pi} \sigma_n} \exp [-\frac{1}{2} (\frac{\mu - \mu_n}{\sigma_n} )^2] d\mu \\
		= \frac{1}{2\pi \sigma \sigma_n} \exp [-\frac{1}{2} \frac{(x-\mu_n)^2}{\sigma^2 + \sigma_n ^2} ] f( \sigma, \sigma _n) \\
		f( \sigma, \sigma_n) = \int \exp [ - \frac{1}{2} \frac{\sigma^2 + \sigma_n ^2} {\sigma^2 \sigma_n ^2} (\mu - \frac{\sigma_n ^2 x + \sigma^2 \mu _n}{ \sigma^2 + \sigma_n ^2}  )^2 ] d \mu
	\end{eqnarray}
	Equation \ref{univariateEvidence} is Gaussian and provides $P(x |D) ~ N(\mu_n , \sigma^2 + \sigma_n ^2)$.  As defined, $P(x |D)$ is the desired class-conditional density $P(x|D_j , \omega_j)$.  Therefore: $P(x | D_j , \omega_j)$ together with $P(\omega_j)$ and using Bayes formula, we obtain a form of a Bayesian classification rule that can be used to design a classifier. 
	\begin{equation}
		\max _{\omega_j} [P(\omega_j | x , D)] = \max _{\omega_j}  P(x | \omega_j , D_j) P(\omega_j)
	\end{equation}
	
\end{itemize}

\subsection{Dogmatism in the univariate case}
%Reference slides of the book (proper credits) 1-11
\label{problem_3_15}





\subsection{MLE compared to Bayesian Esitmation}
By suitable choice of priors in Bayesian learning i.e. 
\begin{eqnarray}
	\mu_0 = \frac{1}{n_0} \sum_{m_0 + 1}^0 x_k + \sigma^2 _0 = \sigma^2 /  n_0 
\end{eqnarray}
In these cases MLE produces equivalent results to Bayesian learning.  

poor model examples problems 3-7 and 3-8  

Part of A (Problem 3-7) : What is the value of the MLE $\hat{\mu}$ in our poor model given a large amount of data?  


Part C, the true model is $~N(1, 10^6)$ will have results shown in image

This has to be solved numerically using 
\begin{eqnarray}
	\frac{1}{\sqrt{2\pi} \sqrt{1}} \exp [(-\frac{x^2}{2} )] = \\
	\frac{1}{\sqrt{2\pi}\sqrt{10^6}}\exp [(-\frac{-(x-1)^2}{2 \times 10^6} )] 
\end{eqnarray}


\section{Bayesian Parameter Estimation: General Theory}

\begin{quote}
	$P(x | D)$ computation can be applied to any situation in which the unknown density can be parameterized.  The basic assumption are: 
	\begin{itemize}
		\item The form of $P(x | \omega)$ is assumed known, but the value of $\theta$ is not known exactly 
		\item Our knowledge about $\theta$ is assumed to be contained in a known prior density $p(\theta)$.
		\item The rest of our knowledge $\theta$ is contained in a set $D$ of $n$ random variable $x_1, x_2,..., x_n$ that follows from $p(x)$.
	\end{itemize} \cite[97]{duda-hart-stork}
\end{quote}

The basic problem is: ``Compute the posterior density $P(\theta | D)$'' then ``Derive $P(x | D)$''
Using equation \ref{class-conditional-posterior-density} $p(\theta |D)$ is derived to equation \ref{class-conditional-posterior-density-general-case} % Bayes Formula, we have 
\begin{quote}
	\begin{equation}
		P( \theta | D) = \frac{ P(D | \theta) P(\theta) } { \int P(D | \theta ) P( \theta) d \theta  } \label{class-conditional-posterior-density-general-case}
	\end{equation}
	And by independence assumption
	\begin{equation}
		P( D | \theta) = \prod _{k=1} ^n P( x_k | \theta)
	\end{equation}
	\cite[97]{duda-hart-stork}
\end{quote}
The wisdom in this cases is that $p(\vec{x} | \mathcal{D}) ~ p(\vec{x} | \hat{\vec{\theta}})$ in cases of a sharp peak and $p(\vec{\theta}) \neq 0$.  All of the information in this case should be used to compute the desired class conditional posterior density $p(\vec{x}| D)$.

Issues for the use of Bayesian Estimation versus Maximum Likelihood stem from the following concerns:
\begin{itemize}
	\item Ease of interpretation which tends to favor maximum likelihood
	\item Ease of computation.  The operations necessary to compute maximum likelihood are far less in general than Bayesian estimation.  (Computation of gradients as opposed to integrals)
	\item Confidence in the training data (the priors).  
	\begin{itemize}
		\item Maximum likelihood requires parameter type class conditionals.  
		\item Bayes estimation does not require class conditionals that are parameter type.
		\item high confidence can ensure better classification, but flat results are similar to MLE.
	\end{itemize}
	\item Asymmetry for broad $p(\vec{\theta}| D)$ yields information about $\hat{\vec{\theta}}$.
	\item Sources of error:
	\begin{itemize}
		\item Bayes or Indistinguishability error
		\item Model Error (cases of incorrect model? is this an error due to wrong distribution,  mistaken parameters, or something else)
		\item Estimation error
	\end{itemize}
	
\end{itemize}
 
\section{Sufficient Statistics}


\section{Problems of Dimensionality} % reference Chapter 3 section 7
\cite[11]{slidesChapter3Part2}
\begin{itemize}
	\item  Problems involving 50 or 100 features (binary valued)
	\begin{itemize}
		\item Classification accuracy depends upon the dimensionality and the amount of training data
		\item Case of two classes multivariate normal with same covariance
		\begin{eqnarray}
		P(\textrm{error}) \frac{1}{\sqrt{2\pi}} \int _{r/2}^{\infty} e^{\frac{u}{2}} du \\
		r^2 = (\mu _1 - \mu_2)^T \Sigma^{-1} (\mu _1 - \mu_2) \\
		\lim _{r \to \infty} P(error) = 0 
		\end{eqnarray}
	\end{itemize}
	\item If features are independent then:  
	\begin{eqnarray}
		\Sigma = \diag ( \sigma_1 ^2, \sigma_2^2 , ..., \sigma_d ^2) \\
		r^2 = \sum _{i=1}^d (\frac{\mu _{i1} - \mu_{i2}}{\sigma_i})^2
	\end{eqnarray}
	\item Most useful features are the ones for which the difference between the means is large relative to the standard deviation.
	\item It has frequently been observed in practice that, beyond a certain point, the inclusion of additional features leads to worse rather than better performance:  \textbf{we have the wrong model!}
	
\end{itemize}


\section{Computational Complexity}

\begin{itemize}
	\item Our design methodology is affected by the computational difficulty : 
	\begin{itemize}
		\item ``big-oh'' notation (order of)
	\end{itemize}
	
\end{itemize}

\subsection{Complexity of the ML Estimation}
\begin{itemize}
	\item Gaussian priors in $d$ dimensions classifier with $n$ training samples for each of $c$ classes 
	\item For each category, we have to compute the discriminant function:
	\begin{equation}
		g(x) = - \frac{1}{2} ( \vec{x} - \mu) ^T  \Sigma^{-1} ( \vec{x} - \mu) - \frac{d}{2}\ln {2\pi} - \frac{1}{2} \ln|\hat{\Sigma} | + \ln P(\omega)
	\end{equation}
	\begin{itemize}
		\item $O(d n)$ for $\hat{\mu}$ 
		\item $O(n d^2)$ for computing $\Sigma ^{-1}$ 
		\item $O(nd^2)$ for computing the determinant of $\Sigma$
		\item $O(n)$ for computing the prior probabilities
		\item Constant complexity for computing the constants.
		\item $O(n d^2)$ tends to dominate the compute cycles 
	\end{itemize}
	
	
\end{itemize}




\end{document}  









