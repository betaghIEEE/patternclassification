\documentclass[11pt]{article}
\usepackage{geometry}                % See geometry.pdf to learn the layout options. There are lots.
\geometry{letterpaper}                   % ... or a4paper or a5paper or ... 
%\geometry{landscape}                % Activate for for rotated page geometry
%\usepackage[parfill]{parskip}    % Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{epstopdf}
\usepackage{algorithm,algorithmic}
\DeclareGraphicsRule{.tif}{png}{.png}{`convert #1 `dirname #1`/`basename #1 .tif`.png}

\title{Notes on PCA in Pattern Classification}
\author{Dan Beatty, Dr. Mitra}
%\date{}                                           % Activate to display a given date or no date

\begin{document}
\maketitle
%\section{}
%\subsection{}


\begin{itemize}
	\item Combine features in order to reduce the dimension of the feature space
	\item Linear combinations are simple to compute and tractable 
	\item Project high dimensional onto a lower dimensional space
	\item Two classical approaches for finding ``optimal'' linear transformation
	\begin{itemize}
		\item Principal Component Analysis ``Projection that best \textbf{represents} the data in a least-square sense.'' 
		\item Multiple Discriminant Analysis ``Projection that bests \textbf{separates} the data in a least squares sense''
	\end{itemize}
	
\end{itemize}

\section{Principle Component Analysis}
Let us have a set of $d$ dimensional vectors $\vec{x_1} , ..., \vec{x_n}$.  We want to represent the set by a single vector $\vec{x_0}$ in such a way that the squared error criterion function:
\begin{eqnarray}
	J_0 (\vec{x_0}) = \sum_{k=1} ^n  ||\vec{x_0} - \vec{x_k} ||^2 \\
	\vec{m} = \frac{1}{n} \sum_{k=1}^n \vec{x_k}
\end{eqnarray}
$\vec{x_k}$ is a zero dimensional representation of the data set.  

For a one-dimensional representation of the data set let us look at a projection of the data onto a line passing through the sample mean.  
\begin{equation}
	\vec{x} = \vec{m} + a \vec{e}
\end{equation}
where $\vec{e}$ is a unit vector in the direction of the line.  
\begin{equation}
\vec{x_k} = m + a_k \vec{e}
\end{equation}
then an optimal set of $a_k$ can be found by minimizing 
\begin{eqnarray}	
J_i (a_1, ..., a_n , e) = \sum _{k=1}^n  || (\vec{m} +a_k \vec{e}) - \vec{x_k}  ||^2 \\
J_i (a_1, ..., a_n , e) = \sum _{k=1}^n  || a_k \vec{e} - (\vec{x_k} - \vec{m}) ||^2 \\
= \sum_{k=1}^n a_k ^2 || \vec{e} || ^2 - 2 \sum _{k=1}^n a_k \vec{e}^T (\vec{x}_k - \vec{m}) + \sum_{k=1}^n || \vec{x_k} - \vec{m} || ^2 
\end{eqnarray}
To minimize $J_1$ we take $\frac{dJ_1}{d a_k} = 0$ and we obtain:
\begin{equation}
a_k = \vec{e}^T (\vec{x_k} - \vec{m})
\end{equation}
which is the least squares solution by projecting $\vec{x_k}$ into a line passing through $\vec{m}$ in the direction of $\vec{e}$. 

A scatter matrix $S$ is defined by 
\begin{equation}
	\mathbf{S} = \sum_{k=1}^n (\vec{x_k} -\vec{m})( \vec{x_k} - \vec{m})^T
\end{equation}
which happens to be the sample covariance $n-1$ times.  

We use it in 
\begin{eqnarray}
J_1 (\vec{e}) = \sum _{k=1}^n a_k ^2 - 2 \sum _{k=1} ^n a_k ^2 + \sum_{k=1}^n || \vec{x_k} - \vec{m} || ^2 \\
= - \sum _{k=1} ^ n | \vec{e}^T ( \vec{x_k} - \vec{m}) |^2  + \sum _{k=1} ^n || \vec{x_k} - \vec{m} || ^2 \\
= - \sum _{k=1} ^n \vec{e}^T ( \vec{x_k} - \vec{m})(\vec{x_T} - \vec{m})^T \vec{e} + \sum_{k=1} ^n || \vec{x_k} - \vec{m} ||^2 \\ 
= -\vec{e}^T \mathbf{S} \vec{e} + \sum_{k=1}^n || \vec{x_k} - \vec{m}||^2
\end{eqnarray}
In order to satisfy the minimal case of $J_1$ using $\vec{e}$, we need to maximize the term $\vec{e}^T \mathbf{S}\vec{e}$.  


Let us use the Lagrange multiplier $\lambda$ subject to the contruct $||e||=1$, 
\begin{eqnarray}
\vec{u} = \vec{e}^T \mathbf{S} \vec{e} - \lambda ( \vec{e}^T\vec{e} - 1) \\
\frac{\partial \vec{u}} {\partial \vec{e}} = 2 \mathbf{S}\vec{e} - 2\lambda \vec{e} \\
\Rightarrow \mathbf{S}\vec{e} = \lambda \vec{e} 
\end{eqnarray}
Applying to $d'$ - dimensional projection such that $d' \le d$
\begin{eqnarray}
	\vec{x} = \vec{m} + \sum_{i=1}^{d'} a_i \vec{e_i} \\
	J_{d'} = \sum_{k=1}^n  || (\sum_{i=1}^{d'} a_i \vec{e_i}) - \vec{x_k} ||^2
\end{eqnarray}
needs to be minimized when the vectors $e_1 , ..., e_{d'}$ are the $d'$ eigenvectors of the scatter matrix $\mathbf{S}$ with the largest eigenvalues. $a_i$ are the principle components of $\vec{x}$ in that basis.  

\section{Fisher's Linear Discriminant}
Discriminant analysis, we need to find projected directions of the data that can discriminate the embedded patterns.  

We have a set of $n$ $d$-dimensional samples $(\vec{x_1},..., \vec{x_n})$ having two subsets $D_1$ and $D_2$, with $n_1$ and $n_2$ samples respectively.  
\begin{equation}
	y = \vec{w}^T \vec{x}
\end{equation}
such that $y$ is a linear combination of the components of $\vec{x}$.

We define corresponding subsets by $Y_1$ and $Y_2$.  If $||\vec{w}||=1$ then each $y_i$ is a projection of $x_i$ onto a line in the direction of $\vec{w}$.
\begin{eqnarray}
	\vec{m_i} = \frac{1}{n_i} \sum_{\vec{x} \in D_i} \vec{x} \\
	\tilde{m_i} = \frac{1}{n_i} \sum_{y \in Y_i} y \\ 
	= \frac{1}{n_i} \frac{\vec{x} \in D_i} {\vec{w}^T \vec{x}} \\
	= \vec{w}^T \vec{m_i}
	\Rightarrow |\tilde{m_1} - \tilde{m_2}|  = | \vec{w}^T (\vec{m_1} - \vec{m_2}) | \label{projected_means_fisher_linear_discriminant} 
\end{eqnarray}
Equation \ref{projected_means_fisher_linear_discriminant} is the projected mean, which is a projection on ${\vec{m_i}}$ \cite[118]{duda-hart-stork}.  

\begin{eqnarray}
\tilde{s_i} ^2 = \sum _{y \in Y_i} (y - \tilde{m_i}) ^2 \label{projected_scatter_fisher_discriminant}\\
\frac{1}{n} (\tilde{s_1}^2 + \tilde{s_2}^2) \label{variance_estimate_scatter_fisher_discriminant} \\
\tilde{s_1}^2 + \tilde{s_2}^2 \label{within_class_scatter_projected_samples} \\
J(\vec{w}) = \frac{| \tilde{m_1} - \tilde{m_2}|^2} {\tilde{s_1}^2 + \tilde{s_2}^2} \label{criterion_function_fisher}
\end{eqnarray}
\begin{itemize}
	\item Equation \ref{projected_scatter_fisher_discriminant} is the scatter for projected samples.
	\item Equation \ref{variance_estimate_scatter_fisher_discriminant} is an estimate of the variance of the pooled data and 
	\item equation \ref{within_class_scatter_projected_samples} is the total within-class scatter.

\end{itemize}
The Fisher Linear discriminant uses the criterion function (equation \ref{criterion_function_fisher}).  

\begin{eqnarray}
	\mathbf{S_i} = \sum _{\vec{x} \in D_i} ( \vec{x} - \vec{m_i})( \vec{x} - \vec{m_i})^T \\
	\mathbf{S_W} = \mathbf{S_1} + \mathbf{S_2} \label{within_class_scatter_matrix} \\
	\tilde{s_i}^2 = \sum _{\vec{x} \in D_i} ( \vec{w}^T \vec{x} - \vec{w}^T \vec{m_i})^2 \\
	= \sum _{\vec{x} \in D_i} \vec{w}^T ( \vec{x} - \vec{m_i})( \vec{x} - \vec{m_i})^T \vec{w} \\
	= \vec{w}^T \mathbf{S_i} \vec{w} \\
	\therefore \tilde{s_1}^2 + \tilde{s_2}^2 = \vec{w}^T \mathbf{S_W}\vec{w} 
\end{eqnarray}

Separation of projected means has its own scatter matrix for which it obeys:
\begin{eqnarray}
(\tilde{m_1} - \tilde{m_2})^2 = ( \vec{w}^T \vec{m_1} - \vec{w}^T \vec{m_2} )^2  \label {projected_means_definition} \\
= \vec{w}^T(  \vec{m_1} -  \vec{m_2} )^2 \vec{w} \\ 
= \vec{w}^T(  \vec{m_1} -  \vec{m_2} )(  \vec{m_1} -  \vec{m_2} )^T \vec{w} \\
= \vec{w}^T \mathbf{S_B} \vec{w} \\
\because \mathbf{S_B} = (\vec{m_1} - \vec{m_2})(\vec{m_1} - \vec{m_2})^T \label {between_class_scatter_matrix}
\end{eqnarray}

\begin{quote}
	In terms of $\mathbf{S_B}$ and $\mathbf{S_W}$, the criterion function $J(\cdot)$ can be written as:
	\begin{equation}
		J(\vec{w}) = \frac{\vec{w}^T \mathbf{S_B} \vec{w} } { \vec{w}^T \mathbf{S_W}\vec{w} } \label{rayleigh_quotient}
	\end{equation}
	
\cite[120]{duda-hart-stork}
\end{quote}
Equation \ref{rayleigh_quotient} is well known as the Rayleigh quotient.  A $\vec{w}$ that minimizing of $J(\vec{w})$ must satisfiy equation \ref{rayleigh_ratio} such that $\lambda$ is a generalized eigenvalue.
\begin{equation}
\mathbf{S_B}\vec{w} = \lambda \mathbf{S_W} \vec{w} \label{rayleigh_ratio}
\end{equation}

If $\mathbf{S_W}$ is non-singular, then equation \ref{fishersDiscriminant} is Fisher's Linear Discriminant.
\begin{equation}
	\vec{w} = \mathbf{S_W} ^{-1} (\vec{m_1} - \vec{m_2}) \label{fishersDiscriminant}
\end{equation}
Equation \ref{fishersDiscriminant} is a mapping from $d$ dimensional to one dimensional classification problem.


%The Fisher's linear discriminant is given by 
	To find the threshold of the point along the mapped one-dimensional subspace separated the projected points, let us assume that the conditional densities $p(x | \omega_i)$ are multivariate normal with equal covariance matrices $\Sigma$ then the optimal decision boundary to given by 
\begin{equation}
	\vec{w}^T \vec{x} + w_0 = 0
\end{equation}
where 
\begin{equation}
\vec{w}= \mathbf{\Sigma}^{-1} ( \vec{\mu_1} - \vec{\mu_2})
\end{equation}
By estimating $\mu_i + \Sigma$ from the sample means and covariances, we can get the direction of $w$ that maximizes $J(\cdot)$. The computational complexity of this approach is mainly due to computing the within-class total scatter and its inverse + involves $O(\alpha^2 n)$ operations.  

\section{MDA}
For $c$- classes problem, we consider the projection for a d-dimensional space to $(c-1)$ dimensional space assuming $d \ge c$
\begin{eqnarray}
	\therefore \mathbf{S_w} = \sum_{i=1}^c \mathbf{S_i} \\
	\mathbf{S_i} =  \sum _{\vec{x} \in D_i} (\vec{x} - \vec{m_i} ) (\vec{x} - \vec{m_i} )^T \\
	\vec{m_i} = \frac{1}{n_i} \sum _{x \in D_i} \vec{x}
\end{eqnarray}

The generalization $\mathbf{S_B}$ is not as direct.  Define a total mean vector $\vec{m}$ and a total scatter matrix $S_T$ by 
\begin{eqnarray}
	\vec{m} = \frac{1}{n} \sum_{\vec{x}} \vec{x} \\
	= \frac{1}{n} \sum_{i=1}^{c} n_i \vec{m_i} \\
	\mathbf{S_T} = \sum _x (x - m)(x - m)^T  \\
	\because \mathbf{S_B} = \sum_{i=1}^c n_i (\vec{m_i} - \vec{m})( \vec{m_i} - \vec{m})^T \\
	\therefore \mathbf{S_T} = \mathbf{S_w} + \sum_{i=1}^c n_i (\vec{m_i} - \vec{m})( \vec{m_i} - \vec{m})^T \\
	= \mathbf{S_w} + \mathbf{S_B}
\end{eqnarray}
The $(c-1)$ discriminant function are given by 
\begin{eqnarray}
	y_i = \vec{w_i} ^T \vec{x} , i = 1, ..., c-1 \\
	\Rightarrow \vec{y} = \mathbf{W}^T \vec{x},
\end{eqnarray}
where $y$ is vector with $y_i$ components and $w$ is a matrix $[d x (c-1)]$ with $w_i$ are the column.

Now 
\begin{eqnarray}
	\tilde{m_i} = \frac{1}{n_i} \sum_{y\in Y_i} y \\
	\tilde{m} = \frac{1}{n} \sum_{i=1}^c n_i \tilde{m_i} \\
	\mathbf{\tilde{S_w}} = \sum_{i=1}^{c} \sum _{y \in Y_i} (y- \tilde{m_i})(y- \tilde{m_i})^T \\
	\mathbf{\tilde{S_B}} = \sum_{i=1}^c n_i (\tilde{m_i} - \tilde{m})(\tilde{m_i} - \tilde{m})^T \\
	\therefore \mathbf{\tilde{S_w}} = \mathbf{W}^T \mathbf{S_W} \mathbf{W} \\
	\mathbf{\tilde{S_B}}= \mathbf{W}^T \mathbf{S_B} \mathbf{W} \\
	J(\mathbf{W}) = \frac{| \tilde{S_B} |} {|\tilde{S_w}|} = \frac{|w^T S_B w |} {|w^T S_B w  |}
\end{eqnarray}

Now $\mathbf{S_B} \mathbf{w_i} = \lambda_i \mathbf{S_W} \mathbf{w_i} $, since the columns of an optimal $\mathbf{W}$ are the generalized eigenvectors corresponding to the largest eigenvalues.  Now we can find the eigenvalues as the roots of the characteristic polynomial 
\begin{equation}
| \mathbf{S_B} - \lambda_i \mathbf{S_w} | = 0
\end{equation}
and solve 
\begin{equation}
(\mathbf{S_B} - \lambda_i \mathbf{S_W}) \vec{w_i} = 0
\end{equation}
for the eigenvectors $\vec{w_i}$.



\begin{algorithm}
\caption{Multiple Discriminant Analysis}
\label{alg:multiple_discriminant_analysis}
\begin{algorithmic}
	\STATE Determine $\vec{m_t}$
	\FORALL{Classes $D_i$ in Discriminant Set $D$}
		\STATE Compute $\vec{m_i}$
		\STATE Determine $n_i$
		\STATE Determine $\hat{m_i} = \vec{m_i} - \vec{m_t}$
		\STATE Compute $S_i = \sum_{\vec{x_i} \in D_i} (\vec{x_i} - \vec{m_i} )(\vec{x_i} - \vec{m_i} )^T $
	\ENDFOR
	\STATE $S_w = \sum _{S_i \in D} S_i$
	\STATE Compute $S_B = \sum_{\hat{m_i} \in D} n_i \hat{m_i}$
	\STATE Compute Top eigenvectors for equation: 
	\[
	\mathbf{S_B} \mathbf{w_i} = \lambda_i \mathbf{S_W} \mathbf{w_i}
	\]
	\RETURN {$\mathbf{W}, \mathbf{\Lambda} $}
\end{algorithmic}
\end{algorithm}

\subsection{Example problem 3-40 \cite[152]{duda-hart-stork}}
Problem statement as read from \cite[152]{duda-hart-stork}.
\begin{quote}
If $S_B$ and $S_w$ are two real, symmetric, d by d matrices, it is well known that there exists a set of $n$ eigenvalues $\lambda_1, ..., \lambda_n$ satisfying $| \mathbf{S_B} - \lambda \mathbf{S_w}| = 0$, with a corresponding set of $n$ eigenvectors, $\vec{e_1},...,\vec{e_n} $ satisfying $\mathbf{S_B}\vec{e_i} = \lambda_i \mathbf{S_w}\vec{e_i}$.  Furthermore, if $\mathbf{S_w}$ is positive definite, the eigenvectors can always be normalized so that $\vec{e_i}^T \mathbf{S_w} \vec{e_i} = \delta_{ij}$ and $\vec{e_i}^T \mathbf{S_B} \vec{e_i} = \delta_{ij}$.  Let $\tilde{\mathbf{S_w}} = \mathbf{W}^T \mathbf{S_w} \mathbf{W}$ and $\tilde{\mathbf{S_B}} =  \mathbf{W}^T \mathbf{S_B} \mathbf{W}$, where $\mathbf{W}$ is a $d$-by-$n$ matrix whose columns correspond to $n$ distinct eigenvectors.  
\begin{enumerate}
	\item Show that $\tilde{\mathbf{S_w}}$ is the $n$-by-$n$ identify matrix $\mathbf{I}$ and that $\tilde{\mathbf{S_B}}$ is a diagonal matrix whose elements are the corresponding eigenvalues.  (This show that the discriminant functions in multiple discriminant analysis analysis are uncorrelated.)
	\item What is the value of $J = \frac{|\mathbf{\tilde{S_B}} |}{|\mathbf{\tilde{S_W}} |}$
	\item Let $\vec{y} = \mathbf{W}^T \vec{x}$ be transformed by scaling the axes with a nonsingular $n$-by-$n$ diagonal matrix $\mathbf{D}$ and by rotating this result with an orthogonal matrix $\mathbf{Q}$ where $\vec{y'} = \mathbf{QD}\vec{y}$.  Show that $J$ is invariant to this transformation.  
\end{enumerate}

\end{quote}



$S_B$ and $S_w$ $\to$ two real, symmetric, $d \times d$ matrices.  Therefore $|S_B - \lambda S_W | = 0$ for a set of $n$ $\lambda$'s and the corresponding $n$ eigenvectors $e_1, ..., e_n$, satisfying 
\begin{equation}
S_B e_1 = \lambda_i S_w e_i
\end{equation}

If $S_w$ is positive definite the eigenvectors can be normalized so that $e_i ^T S_w e_i = \delta _{ij}$ and $e_i^T S_B e_j = \lambda_i \delta_{ij}$.  

Let $\tilde{\mathbf{S_w}} = \mathbf{W}^T \mathbf{S_w} \mathbf{W}$, and $\tilde{\mathbf{S_B}} \mathbf{W}^T \mathbf{S_B} \mathbf{W}$   where $\mathbf{W}$ is a $d \times n$ matrix whose columns correspond to $n$ distinct eigenvectors.
\begin{enumerate}
	\item Show that $\tilde{\mathbf{S_w}} = \mathbf{I}$ (of size $n\times n$) and $\tilde{\mathbf{S_B}} \to$ a diagonal matrix with eigenvalues as diagonal elements.  The discriminant functions in MDA analysis  are uncorrelated.\label{diagonal_proof_problem_3_40}
	\item What is the value of $J \frac{\tilde{\mathbf{S_B}}}{\tilde{\mathbf{S_W}}}$? \label{part_B_problem_3_40}
	\item Let $\vec{y} = \vec{w}^T \vec{x}$ be transformed by scaling the axes with a non-singular $n \times n$ diagonal matrix $D$ and by rotating the result with an orthogonal matrix $\mathbf{Q}$, where $\vec{y'} = \mathbf{Q} \mathbf{D}\vec{y}$.  Show that $J$ is invariant to this transformation.  \label{part_C_problem_3_40}
\end{enumerate}

Answer \ref{diagonal_proof_problem_3_40}, let the set $\{ \}$ are normalized eigenvectors, then $\vec{e_i}^T\mathbf{S_B}\vec{w_i} = \lambda_i \delta_{ij} $ $\vec{e_i}^T \mathbf{S_w}\vec{e_j} = \lambda_i \delta_{ij}$ and the matrix 
\begin{equation}
\mathbf{W} = [ \vec{e_1} , ..., \vec{e_n}]
\end{equation}


Then the within scatter matrix ins:  the now representation is
\begin{eqnarray}
\tilde{S_w} = \mathbf{W}^T \mathbf{S_W} \mathbf{W} = 
\left(
\begin{array}{l}
\vec{e_1} ^T \\
\vdots \\
\vec{e_n} ^T
\end{array}
\right) 
S_w (\vec{e_1} , ..., \vec{e_n} ) \\
=
\left(
\begin{array}{llll}
\vec{e_1} ^T \mathbf{S_W} \vec{e_1} & ... & \vec{e_1}^T \mathbf{S_W} \vec{e_n} \\
\vdots \\
\vec{e_n} ^T \mathbf{S_W} \vec{e_1} & ... & \vec{e_n}^T \mathbf{S_W} \vec{e_n} \\
\end{array}
\right) = I
\end{eqnarray}

Similar the between scatter matrix $S_B$ is estimated as
\begin{eqnarray}
	\tilde{S_B} = \mathbf{W}^T \mathbf{S_B} \mathbf{W} = 
	\left(
	\begin{array}{llll}
	\vec{e_1} ^T \mathbf{S_B} \vec{e_1} & ... & \vec{e_1}^T \mathbf{S_B} \vec{e_n} \\
	\vdots \\
	\vec{e_n} ^T \mathbf{S_B} \vec{e_1} & ... & \vec{e_n}^T \mathbf{S_B} \vec{e_n} \\
	\end{array}
	\right) = 
	\left(
	\begin{array}{ llll}
		\lambda_1 & 0 & \ldots & 0  \\
		\vdots & \lambda _2 & \ddots & \vdots \\
		0 & \ldots & 0 & \lambda_n 
	\end{array}
	\right) \\
	\therefore \tilde{\mathbf{S_w}} = I ~ (n \times n) \\
	\tilde{\mathbf{S_B}} \textrm{ is diagonal containing } \lambda_i 
\end{eqnarray}

Answer \ref{part_B_problem_3_40}, 
\begin{eqnarray}
	|\tilde{\mathbf{S_B}} | = \lambda_1 \lambda_2 ... \lambda_n \\
	| \tilde{\mathbf{S_W}} | = 1 \\
	\therefore J = \lambda_1 \lambda_2 ... \lambda_n
\end{eqnarray}

Answer \ref{part_C_problem_3_40}, Let 
\begin{eqnarray}
\tilde{\mathbf{W}} = \mathbf{QDW}^T \\
\tilde{\mathbf{S_W}} = \tilde{\mathbf{W}} ^T \mathbf{S_W} \tilde{\mathbf{W}} \\
= \mathbf{Q}\mathbf{D}\mathbf{W}^T \mathbf{S_W} \mathbf{WDQ}^T 
\end{eqnarray}
Then: 
\begin{eqnarray}
	| \tilde{S_W} | = |D |^2 \\
	\tilde{\mathbf{S_B}} = \tilde{\mathbf{W}}^T\mathbf{S_B}\tilde{\mathbf{W}} \\
	= \mathbf{Q}\mathbf{D}\mathbf{W}^T \mathbf{S_B} \mathbf{WDQ}^T \\
	\therefore | \tilde{\mathbf{S_B}} | = |D |^2 \lambda_1 \lambda_2 ...\lambda_n \\
	J = \frac{ | \tilde{\mathbf{S_B}} |}{ | \tilde{\mathbf{S_W}}| }  
\end{eqnarray}
Therefore $J$ is invariant to this transformation.




\section{Computer Problems }
	%\cite[155]{duda-hart-stork}  problems 9 and 10.
	
	
Section 3.9 - 3.10 Expectation Maximization  / Hidden Markov Models

Chapter 4 - Non parametric approach   ---


Parsen's Windows ----- 164

Chapter 5  Linear Discriminant Functions ---- 

Chapter 6 

Chapter 7 Stochastic methods


Chapter 8  --- Cart algorithm  


Chapter 10   --- 


\bibliography{../patternNotes.bib}
\bibliographystyle{abbrv}


\end{document}  

