\documentclass[11pt]{article}
\usepackage{geometry}                % See geometry.pdf to learn the layout options. There are lots.
\geometry{letterpaper}                   % ... or a4paper or a5paper or ... 
%\geometry{landscape}                % Activate for for rotated page geometry
%\usepackage[parfill]{parskip}    % Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{epstopdf}
\DeclareGraphicsRule{.tif}{png}{.png}{`convert #1 `dirname #1`/`basename #1 .tif`.png}

\title{Brief Article}
\author{The Author}
%\date{}                                           % Activate to display a given date or no date

\begin{document}
\maketitle
%\section{}
%\subsection{}

ICF for Face recognition

Assumption on basis (Bias of Maximum Likelihood Estimation)

The MLE of the variance $\sigma^2$ is biased 
\begin{equation}
\xi [\frac{1}{n} \sum_{n=1}^n ( x_i - \bar{x})^2] = \frac{n-1}{n}\sigma^2 \neq \sigma^2
\end{equation}
Considering the univariate case, let $\mu$ and $\sigma^2$ be the mean and variance of the Gaussian 
\begin{eqnarray}
\sigma^2 _n = \Xi [\frac{1}{n-1} \sum _{i=1}^n (x_i - \mu)^2] \\
 = \frac{1}{n-1} \Xi [ \sum_{i=1}^n \{(x_i - \mu) - (\hat{\mu} - \mu)  \}^2] \\
\frac{1}{n-1} \Xi [ \sum_{i=1}^n \{(x_i - \mu) -2 (x_i - \mu) (\hat{\mu} - \mu) + (\bar{\mu} - \mu)^2  \}]  \\
 \frac{1}{n-1}[ \sum_{i=1}^n \{\Xi(x_i - \mu) -2 \xi(x_i - \mu) (\hat{\mu} - \mu) + \Xi(\bar{\mu} - \mu)^2  \}] \\
 \frac{1}{n-1}[ \sum_{i=1}^n \{\Xi(x_i - \mu) -2 \xi(x_i - \mu) (\hat{\mu} - \mu) + \Xi(\bar{\mu} - \mu)^2  \}] \\
\\
\Xi [(x_i - \mu)(\hat{\mu} - \mu)]
\end{eqnarray}



The MLE derivation is different.  The unbiased estimator for $\Sigma$ is given by equation 3-21 (page 90).  Where $C$ is the sample covariance matrix and $\hat{\Sigma}$ which equals:
\[
\hat{\Sigma} = \frac{n-1}{n}C 
\]
Asymptotically unbias  (page 90)


PCA issue from hand out document  



From pictures 
\begin{eqnarray}
	E [ (x_i - \mu )(\hat{\mu} - \mu )] \\
	= E[ (x_i - \mu) ( \frac{1}{n} \sum _{j=1}^n (x_j - \mu)  )] \\
	E [ (x_i - \mu) ( \frac{x_i - \mu}{n} + \frac{1}{n} \sum _{k=1, k\neq i}^n x_k - \mu  ) ] \\
	E [ \frac{ (x_i - \mu)^2 }{n}  ] + E [ \frac{ x_i - \mu }{n} \sum_{k=1, k\neq i} ^n (x_k - (n-1)\mu )] \\
	= \frac{\sigma^2}{n} + 0 = \frac{\sigma^2}{n}
	E [ \hat{\mu} - \mu ] = \frac{sigma^2}{n}
	\sigma_n ^2 = \frac{1}{n+1} [ \sigma^2 - \frac{2}{n} \sigma^2 + \frac{\sigma^2} {n}  ] \\
	\frac{n-1}{n-1} \sigma^2 = \sigma^2 \to \textrm{ unbiased}
\end{eqnarray}



Consider the problem of learning the mean of a univariate normal distribution from equations 34 and 35 we have
\begin{eqnarray}
	\mu _n = \frac{n \sigma_0 ^2} {n \sigma_0 ^2 + \sigma^2} m_n + \frac{\sigma^2}{n \sigma_0 ^2 + \sigma^2} \mu_0\\
	\sigma_n ^2 = \frac{ \sigma_0 ^2 \sigma^2 }{ n \sigma_0 ^2 + \sigma^2}
\end{eqnarray}
where $m_n = t_n $

$\mu_0$ is formed by averaging $n_0$ fictitious samples $x_k$ from $k= -n_0 1 , -n_0 +2 , ..., 0$
\begin{eqnarray}
\mu _0 = \frac{1}{n_0} \sum _{-n_0 + 1}{0} x_k \\
\mu_n = \frac{ \sum_{k=1}{n} x_k }{ n + \frac{\sigma^2}{\sigma_0 ^2}} + \frac{ \frac{\sigma^2}{\sigma_0^2}}{\frac{sigma^2}{\sigma_0 ^2} +n} \frac{1}{n_0} \sum _{k=-n_0} ^0 x_k \\
\mu _n =\frac { \sum_{k=1}{n} x_k }{n+n_0} + \frac{n_0}{n+n_0} (\frac{1}{n_0})\sum_{k=-n_0} ^0 x_k \\
n_0 = \frac{\sigma^2} {\sigma_0 ^2} \\
\mu _n = \frac{1}{n+ n_0} [ \sum_{k=1}^n x_k + \sum _{k=n_0} ^0 x_k] \\
=  (\frac{1}{n+ n_0}) \sum_{k= -n_0} ^ n x_k \\
\therefore \sigma_n^2 = {\sigma^2 \sigma_0^2}{n \sigma_0 ^2 + \sigma^2} = \frac{\sigma^2} {n + \frac{\sigma^2}{\sigma_0^2}} \\
= \frac{\sigma^2} {n+ n_0} \neq \sigma^2 
\end{eqnarray}

Problem 3- 7 
What to the value of MLE $\hat{\mu}$ our poor model given a large amount of date?  Note figure:

The true model $~N(1, 10^6)$.


\end{document}  
