%
%  classNotes
%
%  Created by  on 2007-04-17.
%  Copyright (c) 2007 Texas Tech University. All rights reserved.
%
\documentclass[]{article}

% Use utf-8 encoding for foreign characters
\usepackage[utf8]{inputenc}

% Setup for fullpage use
\usepackage{fullpage}

% Uncomment some of the following if you use the features
%
% Running Headers and footers
%\usepackage{fancyhdr}

% Multipart figures
%\usepackage{subfigure}

% More symbols
%\usepackage{amsmath}
%\usepackage{amssymb}
%\usepackage{latexsym}

% Surround parts of graphics with box
\usepackage{boxedminipage}

% Package for including code in the document
\usepackage{listings}

% If you want to generate a toc for each chapter (use with book)
\usepackage{minitoc}

% This is now the recommended way for checking for PDFLaTeX:
\usepackage{ifpdf}

%\newif\ifpdf
%\ifx\pdfoutput\undefined
%\pdffalse % we are not running PDFLaTeX
%\else
%\pdfoutput=1 % we are running PDFLaTeX
%\pdftrue
%\fi

\ifpdf
\usepackage[pdftex]{graphicx}
\else
\usepackage{graphicx}
\fi
\title{A LaTeX Article}
\author{  }

\date{2007-04-17}

\begin{document}

\ifpdf
\DeclareGraphicsExtensions{.pdf, .jpg, .tif}
\else
\DeclareGraphicsExtensions{.eps, .jpg}
\fi

\maketitle


\begin{abstract}
\end{abstract}

\section{ICA Model}
\begin{itemize}

	\item $x = \sum_{i=1}^n a_i s_i$, where $x$ is a random vector with its elements being the mixtures $\vec{x_1}, ..., \vec{x_n}$ and $\vec{s_i}$ is the $i$th, independent component of the random vectors $\vec{s}$, and $a_i$ being the column of the mixing matrix $\mathbf{A}$ so that the ICA model can also be expressed as 

\begin{eqnarray}
\vec{x} = \mathbf{A}\vec{s} \\
\vec{z} = \mathbf{A}^T \vec{w} \\
y = \vec{w}^T \vec{x} = \vec{w}^T \mathbf{A}\vec{s} = \vec{z}^T\vec{s} \label{whittening_ICA}
\end{eqnarray}
	\item The ICA model is a generative model of the observed data $\vec{x}$ from the mixing of the independent source component $\vec{s_i}$.
	\item The source component $\vec{s_i}$ are assumed to be statistically independent must have non-Gaussian distributions. 
	\item We can also obtain the independent components $\vec{s}$ from $\vec{s} = \mathbf{W}\vec{x}$, where $\mathbf{W} = \mathbf{A}^{-1}$ when $\mathbf{A}$ can be estimated and is nonsingular and square.  The matrix $\mathbf{A}$ can not be identified for Gaussian source distributions. The measures of non-Gaussianity are Kurtosis and negentropy.  
\end{itemize}

Ambiguities of ICA:  There are two basic things that cannot be determined via ICA:
\begin{enumerate}
	\item The variances (energies) of the independent components.
	\item The order of the independent components.
\end{enumerate}



Reference ``Quantitative Determination of Distortion in Steganographic Image using Kullback-Leibler Divergence''

\subsection{Measures of Non-Gaussian Distributions}
\subsubsection{Kurtosis}
The Kurtosis of $y$ is defined as 
\begin{equation}
\textrm{kurt}(y) = E \{ y^4 \} -3 ( E \{ y^2 \})^2
\end{equation}
Assuming unit variance for $y$
\begin{equation}
\textrm{kurt}(y) = E \{ y^4 \} - 3
\end{equation}
and is therefore is a normalized version of the fourth moment is $3 \{ E (y^2 ) \} = 0$.  
For most nonGaussian random variables, Kurtosis is non-Gaussian can be positive or negative. 

Random variables with a negative kurtosis are known as sub-Gaussian.  Random variables with a positive kurtosis are known as super-Gaussian.  (Example:  The Laplace distortion) % Reference figure 8 Helsinki
Non-Gaussian is usually measured by the absolute value of Kurtosis and is widely used as a measure of non-Gaussianity in ICA.   

One feature of kurtosis used with regards to independent random variables is a linearity properties:
\begin{eqnarray}
	\textrm{kurt}(x_1 + x_2) = \textrm{kurt}(x_1) + \textrm{kurt}(x_2) \\
	\textrm{kurt}(\alpha x_1) = \alpha^4 \textrm{kurt}(x_1)
\end{eqnarray}

The derivation of kurtosis into the whitening transform used for ICA is derived from equation \ref{whittening_ICA}. 
\begin{eqnarray}
	y = \vec{w}^T \vec{x} \\
	= \vec{w}^T \mathbf{A}\vec{s} \\
	= \vec{z}^T \vec{s} \\
	= z_1 s_1 + z_2 s_2 
\end{eqnarray}
Similarly the application of the kurtosis:
\begin{eqnarray}
	\textrm{kurt}(y) = \textrm{kurt}(z_1 s_1) + \textrm{kurt}(z_2 s_2) \\
	= z_1 ^4 \textrm{kurt}(s_1) + z_2 ^4 \textrm{kurt}(s_2)
\end{eqnarray}

The maxima of such a kurtosis
\begin{equation}
| \textrm{kurt}(y) | = |z_1 ^4 \textrm{kurt}(s_1) + z_2 ^4 \textrm{kurt}(s_2)|
\end{equation}
is constrained to the unit circle and yields a ``optimal landscape''.  The maxima exist when kurtosis is zero and when $y$ is equal to $\plusminus s_i$.  

Kurtosis is sensitive to outliers which can lead to erroneous results.  


\subsubsection{Neg-entropy}
Entropy is a significant concept to ``information theory''.  Entropy is the amount of information observation of a random variable yields.  Entropy, $H(\cdot)$, is defined here in the discrete random variable $Y$ as well as for random vectors $\vec{y}$.
\begin{eqnarray}
	H(Y) = - \sum _i P(Y = a_i) \log P(Y=a_i) \\
	H(\vec{y}) - \int f(\vec{y}) \log f(\vec{y}) d\vec{y}
\end{eqnarray}

One conclusion made from information theory is that a Gaussian variable has the largest entropy among all random variables of equal variance.  

\begin{equation}
J(y) = H ( \textrm{Gaussian}) - H(y) \label{negentropy-defined}
\end{equation}
%$H(\cdot)$ is the entropy.  
Negentropy is defined in equation \ref{negentropy-defined} and   
%Therefore negentropy 
is a modified version of differential entropy.  Properties of negentropy include that it is always non-negative, it is zero only when $y$ is Gaussian.  Although negentropy is an optimal measure of non-Gaussian-ity, estimation of negentropy is computationally intensive since the PDF needs to be estimated.  


\subsubsection{Approximate Negentropy}

To derive an efficient method for ICA, negentropy is approximated by using higher order moments such as 
\begin{equation}
J(y) \approx \frac{1}{12} E \{ y^3 \}^2 + \frac{1}{48} \textrm{kurt}(y)^2
\end{equation}
where the random variable $y$ is assumed to be of zero mean and unit variance.  However, the above approximations suffer from non-robustness inherent to kurtosis.  

To avoid the above problem, measure approximation based on the maximum entropy principle were developed 

In the new approximations, $J(y)$ is expressed as 
\begin{equation}
J(y) \approx \sum _{i=1}^p k_i [E\{ G_i (y)\} - \{ E(G_i(\gamma) ) \}]^2
\end{equation}
\begin{itemize}
	\item where $k_i$'s are positive constants,
	\item $\gamma \to a$ Gaussian  variable of zero mean, and
	\item unit variance, $G_i$'s are non-Gaussian functions
\end{itemize}
Using only one $G$,
\begin{equation}
J(y) [ E \{ G(y) \} - E\{ G(\gamma) \} ]^2
\end{equation}
Taking $G(y) = y^4$, we get a kurtosis based on approximation with 
\begin{eqnarray}
G_1(u) = \frac{1}{a_1} \log \cosh a_i u \\
G_2(u) = - \exp ( - \frac{u^2}{2})
\end{eqnarray}
where $1 \le a_1 \le 2$ is suitable constant.  These approximations of negentropy use a good compromise between two nonGaussianity measures given by kurtosis and negentropy.  

These approximations also allow us to complete ICA fast and provide robustness in addition to being computationally simple.  

\subsection{Defining ICA by mutual information.}

In the mixing matrix model of ICA, the observable $\vec{x}$ is given by $\vec{x} = \mathbf{A} \vec{s}$, where as the independent signal source $\vec{s}$ can be estimated form $\vec{s} = \mathbf{W} \vec{x}$ following blind source seperation method.  

In the mutual information based ICA model, we define the mutual information $I$ between some  random variables $y_i , i = 1, ..., m$.  as 
\begin{equation}
I (y _1, y_2, ..., y_m ) = \sum _{i=1}^m H(y_i) - H(y)
\end{equation}
we also need to have an invertible linear transformation, $\vec{y} = \mathbf{W}\vec{x}$, such that 
\begin{equation}
I(y_1, ..., y_m ) = \sum _{i=1}^m H(y_i ) - H(x) - log | \det W | 
\end{equation}

Further work on $I$ allows 
\begin{eqnarray}
\det \mathbf{I} = 1 = ( \det(\mathbf{W} E\{ \vec{x} \vec{x}^T\} \mathbf{W^T})) \\
= (\det \mathbf{W}) (\det E \{ \vec{x}\vec{x}^T \}) (\det \{\mathbf{W}^T \}) \\
I(y_1, y_2, ..., y_n) = C - \sum_i J(y_i) \label{mutual_information_negentropy_link}
\end{eqnarray}
such that $C$ is a constant not dependent on $\mathbf{W}$


% Reference section 4.3.2 Helsinki
``It is now obvious from equation \ref{mutual_information_negentropy_link} that finding an invertible transformation $\mathbf{W}$  that minimizes the mutual information is roughly equivalent to finding directions in which the negentropy is maximized. '' \cite[10]{helsinki}


\begin{quote}
A very popular approach for estimating the ICA model is maximum likelihood estimation, which is closely connected to the infomax principle. \cite[10]{helsinki}
\end{quote}


\begin{quote}
	The goal of ICA can be understood in the domain of blind source separation.  Suppose there are $d$ independent scalar source signals $x_1 (t)$ for $i = 1, ..., d$, where we can consider $t$ to be a time index $1 \le t \le T$. %...   The mean of $\vec{x}$ 
\cite[570]{duda-hart-stork}
\end{quote}

\subsection{MLE variety of ICA}

One claim, it is popular estimate ICA model is maximum-likelihood estimation.  The log-likelihood equation is as in equation \ref{mle_ica}.  
\begin{equation}
	L = \sum_{t=1} ^{T} \sum_{i=1}^n \log f_i (\vec{w}_i^T \vec{x}(t)) + T \log | \det \mathbf{W} | \label{mle_ica}
\end{equation}
In this equation, the following notation needs explaining:
\begin{itemize}
	\item $f_i$ is the density $s_i$ (assumed known )
	\item $\vec{x}(t)$ are realizations of $\vec{x}$
	\item The term $\log | \det \mathbf{W}|$ comes from the classic rule for transforming random variables and their densities. 
\end{itemize}

Note that if the densities of MLE are not estimated correctly, the results will simply be wrong.  

%Refer to figure 10.25 in Duda-Hart-Stork
The key to the infomax principle is spelled out in equation \ref{infomax-helsinki}
\begin{equation}
L_2 = H ( \phi _1 (\vec{w}_1^T), ..., \phi_n ( \vec{w}_n ^T \vec{x})) \label{infomax-helsinki}
\end{equation}
Each of the $\phi(\cdot)$ are non-linear functions.  The $\vec{w}_i$ are weight vectors.  
% Further reference to ICA and Projection Pursuit page 11 Helsinki


\subsection{Projection Pursuit}
\begin{itemize}

	\item ``Projection pursuit is a technique developed in statistics for finding interesting projections of multidimensional data.''  
	\item Are Gaussian distributions the least interesting set of distributions?  Some claim that the less Gaussian, the more interesting.  
	\item ``All the nonGaussianity measures and the corresponding ICA algorithms presented here could also be called projection pursuit indices and algorithms.''
	\item Project pursuits allow computation of $s_i$ when there are less $\vec{s}_i$ than there are $\vec{x}_i$, such that the rest is filled with Gaussian noise.
	\item If the ICA models holds, optimizing the ICA non-Gaussianity measures produce independent components;  if the model does not hold, then what we get are the projection pursuit directions.
\end{itemize}



\section{Preprocessing for ICA}

There are few steps of preprocessing applied to the data.  
\begin{enumerate}
	\item Transform zero-mean variable 
	\begin{eqnarray}
		\vec{m} = E \{ \vec{x}\} \\
		\vec{s} \to \mathbf{A}^{-1}\vec{m}
	\end{eqnarray}
	\item Apply the whitening transform to the $\vec{x}$ via Eigen-value Decomposition
	\begin{eqnarray}
	E\{ \tilde{x}\tilde{x}^T \} = \mathbf{I} \\
	\tilde{x} = \mathbf{E}\mathbf{D}^{-1/2}\mathfb{E}^T \vec{x} \\
	= \mathbf{E}\mathbf{D}^{-1/2}\mathfb{E}^T \mathbf{A}\vec{s} \\
	\end{eqnarray}
	
\end{enumerate}





\bibliography{}
\end{document}
